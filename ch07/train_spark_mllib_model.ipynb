{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()#\n",
    "conf.set(\"spark.python.profile\", \"true\")\n",
    "#conf = sc.getConf()\n",
    "conf.set( \"spark.driver.memory\", \"4g\" )\n",
    "conf.set( \"spark.jars\",\"/Users/crisliu/agile/lib/mongo-hadoop-spark-2.0.2.jar,\\\n",
    "/Users/crisliu/agile/lib/mongo-java-driver-3.4.0.jar,\\\n",
    "/Users/crisliu/agile/lib/elasticsearch-spark-20_2.10-5.0.0-alpha5.jar,\\\n",
    "/Users/crisliu/agile/lib/snappy-java-1.1.2.6.jar,\\\n",
    "/Users/crisliu/agile/lib/lzo-hadoop-1.0.5.jar\")\n",
    "#/Users/crisliu/agile/lib/elasticsearch-hadoop-5.0.0-alpha5.jar\"\n",
    "#conf.set(\"spark.executor.extraClassPath\",\"~\")\n",
    "sc = SparkContext('local', 'test', conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "APP_NAME = \"my_script.py\"\n",
    "spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/Users/crisliu/agile/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ArrDelay=13.0, CRSArrTime=datetime.datetime(2015, 1, 1, 13, 10), CRSDepTime=datetime.datetime(2015, 1, 1, 10, 30), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=14.0, Dest='DFW', Distance=569.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='1024', Origin='ABQ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# {\n",
    "#   \"ArrDelay\":5.0,\"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\",\"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\",\n",
    "#   \"Carrier\":\"WN\",\"DayOfMonth\":31,\"DayOfWeek\":4,\"DayOfYear\":365,\"DepDelay\":14.0,\"Dest\":\"SAN\",\"Distance\":368.0,\n",
    "#   \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\",\"FlightNum\":\"6109\",\"Origin\":\"TUS\"\n",
    "# }\n",
    "#\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])\n",
    "\n",
    "input_path = \"{}/Agile_Data_Code_2/data/simple_flight_delay_features.jsonl.bz2\".format(\n",
    "base_path\n",
    ")\n",
    "features = spark.read.json(input_path, schema=schema)\n",
    "features.first()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check for nulls in features before using Spark ML\n",
    "#\n",
    "null_counts = [(column, features.where(features[column].isNull()).count()) for column in features.columns]\n",
    "cols_with_nulls = filter(lambda x: x[1] > 0, null_counts)\n",
    "print(list(cols_with_nulls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+\n",
      "|    13.0|2015-01-01 13:10:...|2015-01-01 10:30:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1024|   ABQ|ABQ-DFW|\n",
      "|    17.0|2015-01-01 05:15:...|2015-01-01 02:25:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1184|   ABQ|ABQ-DFW|\n",
      "|    36.0|2015-01-01 06:45:...|2015-01-01 04:00:...|     AA|         1|        4|        1|    -2.0| DFW|   569.0|2014-12-31|      336|   ABQ|ABQ-DFW|\n",
      "|   -21.0|2015-01-01 14:30:...|2015-01-01 12:55:...|     AA|         1|        4|        1|    -1.0| DFW|   731.0|2014-12-31|      125|   ATL|ATL-DFW|\n",
      "|   -14.0|2015-01-01 05:25:...|2015-01-01 03:55:...|     AA|         1|        4|        1|    -4.0| DFW|   731.0|2014-12-31|     1455|   ATL|ATL-DFW|\n",
      "|    16.0|2015-01-01 10:15:...|2015-01-01 08:45:...|     AA|         1|        4|        1|    15.0| DFW|   731.0|2014-12-31|     1473|   ATL|ATL-DFW|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Add a Route variable to replace FlightNum\n",
    "#\n",
    "from pyspark.sql.functions import lit, concat\n",
    "features_with_route = features.withColumn(\n",
    "'Route',\n",
    "concat(\n",
    "  features.Origin,\n",
    "  lit('-'),\n",
    "  features.Dest\n",
    ")\n",
    ")\n",
    "features_with_route.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|    13.0|           2.0|\n",
      "|    17.0|           2.0|\n",
      "|    36.0|           3.0|\n",
      "|   -21.0|           0.0|\n",
      "|   -14.0|           1.0|\n",
      "|    16.0|           2.0|\n",
      "|    -7.0|           1.0|\n",
      "|    13.0|           2.0|\n",
      "|    25.0|           2.0|\n",
      "|    58.0|           3.0|\n",
      "|    14.0|           2.0|\n",
      "|     1.0|           2.0|\n",
      "|   -29.0|           0.0|\n",
      "|   -10.0|           1.0|\n",
      "|    -3.0|           1.0|\n",
      "|    -8.0|           1.0|\n",
      "|    -1.0|           1.0|\n",
      "|   -14.0|           1.0|\n",
      "|   -16.0|           0.0|\n",
      "|    18.0|           2.0|\n",
      "+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Use pysmark.ml.feature.Bucketizer to bucketize ArrDelay into on-time, slightly late, very late (0, 1, 2)\n",
    "#\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Setup the Bucketizer\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "arrival_bucketizer = Bucketizer(\n",
    "splits=splits,\n",
    "inputCol=\"ArrDelay\",\n",
    "outputCol=\"ArrDelayBucket\"\n",
    ")\n",
    "\n",
    "\n",
    "# Apply the bucketizer\n",
    "ml_bucketized_features = arrival_bucketizer.transform(features_with_route)\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ArrDelay: double, CRSArrTime: timestamp, CRSDepTime: timestamp, Carrier: string, DayOfMonth: int, DayOfWeek: int, DayOfYear: int, DepDelay: double, Dest: string, Distance: double, FlightDate: date, FlightNum: string, Origin: string, Route: string, ArrDelayBucket: double]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_bucketized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier\n",
      "Origin\n",
      "Dest\n",
      "Route\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Extract features tools in with pyspark.ml.feature\n",
    "#\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Turn category fields into indexes\n",
    "for column in [\"Carrier\", \"Origin\", \"Dest\", \"Route\"]:\n",
    "    print(column)\n",
    "    string_indexer = StringIndexer(\n",
    "          inputCol=column,\n",
    "          outputCol=column + \"_index\"\n",
    "        )\n",
    "\n",
    "    string_indexer_model = string_indexer.fit(ml_bucketized_features)\n",
    "    ml_bucketized_features = string_indexer_model.transform(ml_bucketized_features)\n",
    "\n",
    "    # Drop the original column\n",
    "    ml_bucketized_features = ml_bucketized_features.drop(column)\n",
    "    \n",
    "    # Save the pipeline model\n",
    "    string_indexer_output_path = \"{}/models/string_indexer_model_{}.bin\".format(\n",
    "      base_path,\n",
    "      column\n",
    "    )\n",
    "    string_indexer_model.write().overwrite().save(string_indexer_output_path)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+---------+---------+--------+--------+----------+---------+--------------+--------------------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Distance|FlightDate|FlightNum|ArrDelayBucket|        Features_vec|\n",
      "+--------+--------------------+--------------------+----------+---------+---------+--------+--------+----------+---------+--------------+--------------------+\n",
      "|    13.0|2015-01-01 13:10:...|2015-01-01 10:30:...|         1|        4|        1|    14.0|   569.0|2014-12-31|     1024|           2.0|[14.0,569.0,1.0,4...|\n",
      "|    17.0|2015-01-01 05:15:...|2015-01-01 02:25:...|         1|        4|        1|    14.0|   569.0|2014-12-31|     1184|           2.0|[14.0,569.0,1.0,4...|\n",
      "|    36.0|2015-01-01 06:45:...|2015-01-01 04:00:...|         1|        4|        1|    -2.0|   569.0|2014-12-31|      336|           3.0|[-2.0,569.0,1.0,4...|\n",
      "|   -21.0|2015-01-01 14:30:...|2015-01-01 12:55:...|         1|        4|        1|    -1.0|   731.0|2014-12-31|      125|           0.0|[-1.0,731.0,1.0,4...|\n",
      "|   -14.0|2015-01-01 05:25:...|2015-01-01 03:55:...|         1|        4|        1|    -4.0|   731.0|2014-12-31|     1455|           1.0|[-4.0,731.0,1.0,4...|\n",
      "|    16.0|2015-01-01 10:15:...|2015-01-01 08:45:...|         1|        4|        1|    15.0|   731.0|2014-12-31|     1473|           2.0|[15.0,731.0,1.0,4...|\n",
      "|    -7.0|2015-01-01 07:15:...|2015-01-01 05:45:...|         1|        4|        1|    -2.0|   731.0|2014-12-31|     1513|           1.0|[-2.0,731.0,1.0,4...|\n",
      "|    13.0|2015-01-01 11:50:...|2015-01-01 10:25:...|         1|        4|        1|     9.0|   731.0|2014-12-31|      194|           2.0|[9.0,731.0,1.0,4....|\n",
      "|    25.0|2015-01-01 15:30:...|2015-01-01 14:00:...|         1|        4|        1|    -2.0|   731.0|2014-12-31|      232|           2.0|[-2.0,731.0,1.0,4...|\n",
      "|    58.0|2015-01-01 16:40:...|2015-01-01 15:15:...|         1|        4|        1|    14.0|   731.0|2014-12-31|      276|           3.0|[14.0,731.0,1.0,4...|\n",
      "|    14.0|2015-01-01 08:25:...|2015-01-01 06:55:...|         1|        4|        1|    15.0|   731.0|2014-12-31|      314|           2.0|[15.0,731.0,1.0,4...|\n",
      "|     1.0|2015-01-01 13:05:...|2015-01-01 11:40:...|         1|        4|        1|    -5.0|   731.0|2014-12-31|      356|           2.0|[-5.0,731.0,1.0,4...|\n",
      "|   -29.0|2015-01-01 05:12:...|2015-01-01 03:15:...|         1|        4|        1|    -9.0|   594.0|2014-12-31|     1652|           0.0|[-9.0,594.0,1.0,4...|\n",
      "|   -10.0|2015-01-01 03:52:...|2015-01-01 02:00:...|         1|        4|        1|    -4.0|   594.0|2014-12-31|       17|           1.0|[-4.0,594.0,1.0,4...|\n",
      "|    -3.0|2015-01-01 18:02:...|2015-01-01 16:10:...|         1|        4|        1|    -7.0|   594.0|2014-12-31|      349|           1.0|[-7.0,594.0,1.0,4...|\n",
      "|    -8.0|2015-01-01 09:35:...|2015-01-01 08:30:...|         1|        4|        1|    -2.0|   190.0|2014-12-31|     1023|           1.0|[-2.0,190.0,1.0,4...|\n",
      "|    -1.0|2015-01-01 01:50:...|2015-01-01 00:50:...|         1|        4|        1|    -2.0|   190.0|2014-12-31|     1178|           1.0|[-2.0,190.0,1.0,4...|\n",
      "|   -14.0|2015-01-01 04:40:...|2015-01-01 03:30:...|         1|        4|        1|    -6.0|   190.0|2014-12-31|     1296|           1.0|[-6.0,190.0,1.0,4...|\n",
      "|   -16.0|2015-01-01 05:15:...|2015-01-01 04:05:...|         1|        4|        1|    -4.0|   190.0|2014-12-31|     1356|           0.0|[-4.0,190.0,1.0,4...|\n",
      "|    18.0|2015-01-01 11:55:...|2015-01-01 10:55:...|         1|        4|        1|     3.0|   190.0|2014-12-31|     1365|           2.0|[3.0,190.0,1.0,4....|\n",
      "+--------+--------------------+--------------------+----------+---------+---------+--------+--------+----------+---------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine continuous, numeric fields with indexes of nominal ones\n",
    "# ...into one feature vector\n",
    "numeric_columns = [\n",
    "\"DepDelay\", \"Distance\",\n",
    "\"DayOfMonth\", \"DayOfWeek\",\n",
    "\"DayOfYear\"]\n",
    "index_columns = [\"Carrier_index\", \"Origin_index\",\n",
    "               \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "inputCols=numeric_columns + index_columns,\n",
    "outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "\n",
    "# Drop the index columns\n",
    "for column in index_columns:\n",
    "    final_vectorized_features = final_vectorized_features.drop(column)\n",
    "\n",
    "# Inspect the finalized features\n",
    "final_vectorized_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit random forest classifier on all the data\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "featuresCol=\"Features_vec\",\n",
    "labelCol=\"ArrDelayBucket\",\n",
    "predictionCol=\"Prediction\",\n",
    "maxBins=4657,\n",
    "maxMemoryInMB=1024\n",
    ")\n",
    "model = rfc.fit(final_vectorized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5948777110567574\n",
      "+----------+-------+\n",
      "|Prediction|  count|\n",
      "+----------+-------+\n",
      "|       0.0|    139|\n",
      "|       1.0|4158208|\n",
      "|       3.0| 582953|\n",
      "|       2.0| 972708|\n",
      "+----------+-------+\n",
      "\n",
      "+--------+--------------------+--------------------+----------+---------+---------+--------+--------+----------+---------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Distance|FlightDate|FlightNum|ArrDelayBucket|        Features_vec|       rawPrediction|         probability|Prediction|\n",
      "+--------+--------------------+--------------------+----------+---------+---------+--------+--------+----------+---------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|   -25.0|2015-01-01 06:07:...|2015-01-01 02:30:...|         1|        4|        1|    -4.0|  1237.0|2014-12-31|      569|           0.0|[-4.0,1237.0,1.0,...|[6.47997233107338...|[0.32399861655366...|       1.0|\n",
      "|   -18.0|2015-01-01 04:25:...|2015-01-01 02:39:...|         1|        4|        1|    -5.0|   991.0|2014-12-31|      787|           0.0|[-5.0,991.0,1.0,4...|[5.61755232954080...|[0.28087761647704...|       1.0|\n",
      "|   -16.0|2015-01-01 04:00:...|2015-01-01 02:50:...|         1|        4|        1|    -5.0|   236.0|2014-12-31|      474|           0.0|[-5.0,236.0,1.0,4...|[3.67427023416190...|[0.18371351170809...|       1.0|\n",
      "|   -22.0|2015-01-01 05:42:...|2015-01-01 03:51:...|         1|        4|        1|     7.0|   537.0|2014-12-31|     5407|           0.0|[7.0,537.0,1.0,4....|[2.29723945856528...|[0.11486197292826...|       2.0|\n",
      "|     7.0|2015-01-01 09:35:...|2015-01-01 06:40:...|         1|        4|        1|    -4.0|  1865.0|2014-12-31|      705|           2.0|[-4.0,1865.0,1.0,...|[6.13493216670421...|[0.30674660833521...|       1.0|\n",
      "|   -12.0|2015-01-01 11:35:...|2015-01-01 10:00:...|         1|        4|        1|     0.0|   444.0|2014-12-31|     2272|           1.0|[0.0,444.0,1.0,4....|[1.84177897555947...|[0.09208894877797...|       1.0|\n",
      "+--------+--------------------+--------------------+----------+---------+---------+--------+--------+----------+---------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test data\n",
    "predictions = model.transform(final_vectorized_features)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "predictionCol=\"Prediction\",\n",
    "labelCol=\"ArrDelayBucket\",\n",
    "metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "# Check the distribution of predictions\n",
    "predictions.groupBy(\"Prediction\").count().show()\n",
    "\n",
    "# Check a sample\n",
    "predictions.sample(False, 0.001, 18).orderBy(\"CRSDepTime\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the pipeline model\n",
    "string_indexer_output_path = \"{}/models/string_indexer_model_{}.bin\".format(\n",
    "  base_path,\n",
    "  column\n",
    ")\n",
    "string_indexer_model.write().overwrite().save(string_indexer_output_path)\n",
    "# Save the numeric vector assembler\n",
    "vector_assembler_path = \"{}/models/numeric_vector_assembler.bin\".format(base_path)\n",
    "vector_assembler.write().overwrite().save(vector_assembler_path)\n",
    "\n",
    "# Save the bucketizer\n",
    "arrival_bucketizer_path = \"{}/models/arrival_bucketizer_2.0.bin\".format(base_path)\n",
    "arrival_bucketizer.write().overwrite().save(arrival_bucketizer_path)\n",
    "# Save the new model over the old one\n",
    "model_output_path = \"{}/models/spark_random_forest_classifier.flight_delays.5.0.bin\".format(base_path)\n",
    "model.write().overwrite().save(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
