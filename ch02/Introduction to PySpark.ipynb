{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()#\n",
    "conf.set(\"spark.python.profile\", \"true\")\n",
    "#conf = sc.getConf()\n",
    "conf.set( \"spark.driver.memory\", \"4g\" )\n",
    "conf.set( \"spark.jars\",\"/Users/crisliu/agile/lib/mongo-hadoop-spark-2.0.2.jar,\\\n",
    "/Users/crisliu/agile/lib/mongo-java-driver-3.4.0.jar,\\\n",
    "/Users/crisliu/agile/lib/elasticsearch-spark-20_2.10-5.0.0-alpha5.jar,\\\n",
    "/Users/crisliu/agile/lib/snappy-java-1.1.2.6.jar,\\\n",
    "/Users/crisliu/agile/lib/lzo-hadoop-1.0.5.jar\")\n",
    "#/Users/crisliu/agile/lib/elasticsearch-hadoop-5.0.0-alpha5.jar\"\n",
    "#conf.set(\"spark.executor.extraClassPath\",\"~\")\n",
    "sc = SparkContext('local', 'test', conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "APP_NAME = \"my_script.py\"\n",
    "spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo_spark\n",
    "# Important: activate pymongo_spark \n",
    "pymongo_spark.activate()\n",
    "csv_lines = sc.textFile(\"/Users/crisliu/agile/Agile_Data_Code_2/data/example.csv\")\n",
    "data = csv_lines.map(lambda line: line.split(\",\")) \n",
    "schema_data = data.map(\n",
    "lambda x: {'name': x[0], 'company': x[1], 'title': x[2]} )\n",
    "schema_data.saveToMongoDB('mongodb://localhost:27017/agile_data_science.executives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5819079"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_time_dataframe = spark.read.parquet('/Users/crisliu/agile/Agile_Data_Code_2/data/on_time_performance.parquet')\n",
    "on_time_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bc8474b78bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note we have to convert the row to a dict to avoid https://jira.mongodb.org/browse/HADOOP-276\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mas_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mon_time_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mas_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveToMongoDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mongodb://localhost:27017/agile_data_science.on_time_performance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/agile/lib/pymongo_spark.py\u001b[0m in \u001b[0;36msaveToMongoDB\u001b[0;34m(self, connection_string, config)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'com.mongodb.spark.pickle.NoopConverter'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'com.mongodb.spark.pickle.NoopConverter'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         conf=conf)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsNewAPIHadoopFile\u001b[0;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf)\u001b[0m\n\u001b[1;32m   1419\u001b[0m                                                        \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m                                                        \u001b[0mkeyClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m                                                        keyConverter, valueConverter, jconf)\n\u001b[0m\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Note we have to convert the row to a dict to avoid https://jira.mongodb.org/browse/HADOOP-276\n",
    "as_dict = on_time_dataframe.rdd.map(lambda row: row.asDict())\n",
    "as_dict.saveToMongoDB('mongodb://localhost:27017/agile_data_science.on_time_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o76.save.\n: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[127.0.0.1:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:431)\n\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:439)\n\tat org.elasticsearch.hadoop.rest.RestClient.delete(RestClient.java:495)\n\tat org.elasticsearch.hadoop.rest.RestRepository.delete(RestRepository.java:422)\n\tat org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:481)\n\tat org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:76)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:426)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-261433b21859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mon_time_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.elasticsearch.spark.sql\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.resource\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"agile_data_science/on_time_performance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.batch.size.entries\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.save.\n: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[127.0.0.1:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:431)\n\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:439)\n\tat org.elasticsearch.hadoop.rest.RestClient.delete(RestClient.java:495)\n\tat org.elasticsearch.hadoop.rest.RestRepository.delete(RestRepository.java:422)\n\tat org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:481)\n\tat org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:76)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:426)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "a = on_time_dataframe.write.format(\"org.elasticsearch.spark.sql\")\\\n",
    ".option(\"es.resource\",\"agile_data_science/on_time_performance\")\\\n",
    ".option(\"es.batch.size.entries\",\"100\")\\\n",
    ".mode(\"overwrite\")\n",
    "a.save()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n: org.apache.spark.SparkException: RDD element of type java.util.HashMap cannot be used\n\tat org.apache.spark.api.python.SerDeUtil$.pythonToPairRDD(SerDeUtil.scala:238)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:827)\n\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-996bc3020bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mkeyClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.apache.hadoop.io.NullWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalueClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.elasticsearch.hadoop.mr.LinkedMapWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   conf={ \"es.resource\" : \"agile_data_science/executives\" })\n\u001b[0m",
      "\u001b[0;32m~/agile/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsNewAPIHadoopFile\u001b[0;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf)\u001b[0m\n\u001b[1;32m   1419\u001b[0m                                                        \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m                                                        \u001b[0mkeyClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m                                                        keyConverter, valueConverter, jconf)\n\u001b[0m\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n: org.apache.spark.SparkException: RDD element of type java.util.HashMap cannot be used\n\tat org.apache.spark.api.python.SerDeUtil$.pythonToPairRDD(SerDeUtil.scala:238)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:827)\n\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "schema_data.saveAsNewAPIHadoopFile(\n",
    "  path='-', \n",
    "  outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "  keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "  valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "  conf={ \"es.resource\" : \"agile_data_science/executives\" })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:elasticsearch:GET /agile_data_science.flights_per_airplane/_search?q=name%3AAA [status:404 request:0.005s]\n"
     ]
    },
    {
     "ename": "ElasticHttpNotFoundError",
     "evalue": "(404, 'index_not_found_exception')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pyelasticsearch/client.py\u001b[0m in \u001b[0;36msend_request\u001b[0;34m(self, method, path_components, body, query_params)\u001b[0m\n\u001b[1;32m    275\u001b[0m                             for k, v in iteritems(query_params)),\n\u001b[0;32m--> 276\u001b[0;31m                 body=body)\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSerializationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_request_fail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTP_EXCEPTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: TransportError(404, 'index_not_found_exception')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mElasticHttpNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fa5ab451d7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyelasticsearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElasticSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://localhost:9200/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name:AA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'agile_data_science.flights_per_airplane'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pyelasticsearch/client.py\u001b[0m in \u001b[0;36mdecorate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconvertible_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mquery_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pyelasticsearch/client.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melastic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0melasticsearch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0m_the_search_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_or_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_search'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mes_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'analyzer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default_operator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'routing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pyelasticsearch/client.py\u001b[0m in \u001b[0;36m_search_or_count\u001b[0;34m(self, kind, query, index, doc_type, query_params)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             query_params=query_params)\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mes_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'routing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pyelasticsearch/client.py\u001b[0m in \u001b[0;36msend_request\u001b[0;34m(self, method, path_components, body, query_params)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprepped_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pyelasticsearch/client.py\u001b[0m in \u001b[0;36m_raise_exception\u001b[0;34m(self, status, error_message)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexAlreadyExistsError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mElasticHttpNotFoundError\u001b[0m: (404, 'index_not_found_exception')"
     ]
    }
   ],
   "source": [
    "from pyelasticsearch import ElasticSearch\n",
    "es = ElasticSearch('http://localhost:9200/')\n",
    "es.search('name:AA', index='agile_data_science.flights_per_airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.create_index(index='agile_data_science.flights_per_airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [], 'max_score': None, 'total': 0},\n",
       " 'timed_out': False,\n",
       " 'took': 2}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search('TailNum: N5FNAA', index='agile_data_science.flights_per_airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year='2015', Quarter='1', Month='1', DayofMonth='1', DayOfWeek='4', FlightDate='2015-01-01', Carrier='AA', TailNum='N001AA', FlightNum='1519', Origin='DFW', OriginCityName='Dallas/Fort Worth, TX', OriginState='TX', Dest='MEM', DestCityName='Memphis, TN', DestState='TN', DepTime='1342', DepDelay=-3.0, DepDelayMinutes=0, TaxiOut=16.0, TaxiIn=7.0, WheelsOff='1358', WheelsOn='1457', ArrTime='1504', ArrDelay=-6.0, ArrDelayMinutes=0.0, Cancelled=0, Diverted=0, ActualElapsedTime=82.0, AirTime=59.0, Flights=1, Distance=432.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None, CRSDepTime='1345', CRSArrTime='1510')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_time_dataframe.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL to look at the total flights by month across 2015\n",
    "on_time_dataframe.registerTempTable(\"on_time_dataframe\")\n",
    "total_flights_by_month = spark.sql(\n",
    "  \"\"\"SELECT Month, Year, COUNT(*) AS total_flights\n",
    "  FROM on_time_dataframe\n",
    "  GROUP BY Year, Month\n",
    "  ORDER BY Year, Month\"\"\"\n",
    ")\n",
    "\n",
    "# This map/asDict trick makes the rows print a little prettier. It is optional.\n",
    "flights_chart_data = total_flights_by_month.rdd.map(lambda row: row.asDict())\n",
    "flights_chart_data.collect()\n",
    "\n",
    "# Save chart to MongoDB\n",
    "import pymongo_spark\n",
    "pymongo_spark.activate()\n",
    "flights_chart_data.saveToMongoDB(\n",
    "  'mongodb://localhost:27017/agile_data_science.flights_by_month'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flights': [('AA', '2015-01-02', '1582', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-01-02', '1582', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-01-02', '2440', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-01-03', '1541', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-01-03', '1605', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-01-04', '2278', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-01-05', '1582', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-01-05', '1582', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-01-14', '369', 'DFW', 'SAN'),\n",
       "  ('AA', '2015-01-14', '369', 'SAN', 'DFW'),\n",
       "  ('AA', '2015-01-15', '2441', 'DFW', 'LAS'),\n",
       "  ('AA', '2015-01-15', '2441', 'LAS', 'DFW'),\n",
       "  ('AA', '2015-01-16', '1533', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-01-16', '1533', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-01-16', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-01-18', '2292', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-01-19', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-01-19', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-01-19', '2387', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-01-20', '1260', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-01-20', '1646', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-01-21', '1260', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-01-21', '2351', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-01-22', '1605', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-01-23', '1508', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-01-23', '2278', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-01-29', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-01-29', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-01-31', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-01-31', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-02-01', '2351', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-02-07', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-02-13', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-02-13', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-02-20', '2351', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-02-24', '2292', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-02-25', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-02-25', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-02-25', '2387', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-02-26', '1582', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-02-26', '1582', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-02-27', '1605', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-02-28', '1180', 'DFW', 'SJU'),\n",
       "  ('AA', '2015-02-28', '1571', 'SJU', 'MIA'),\n",
       "  ('AA', '2015-02-28', '2278', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-03-02', '1196', 'EGE', 'MIA'),\n",
       "  ('AA', '2015-03-02', '1196', 'MIA', 'EGE'),\n",
       "  ('AA', '2015-03-06', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-03-12', '1605', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-03-13', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-03-13', '2278', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-03-18', '2351', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-03-20', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-03-20', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-03-20', '397', 'MIA', 'SJU'),\n",
       "  ('AA', '2015-03-21', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-03-21', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-03-26', '1275', 'STT', 'JFK'),\n",
       "  ('AA', '2015-03-26', '1320', 'JFK', 'STT'),\n",
       "  ('AA', '2015-03-29', '134', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-03-29', '1345', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-03-30', '1380', 'JFK', 'STT'),\n",
       "  ('AA', '2015-03-30', '1380', 'STT', 'JFK'),\n",
       "  ('AA', '2015-04-02', '2351', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-04-03', '1218', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-04-03', '1218', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-04-04', '1684', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-04-04', '1684', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-04-06', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-04-08', '2433', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-04-09', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-04-09', '1533', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-04-11', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-04-14', '1048', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-04-16', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-04-17', '1125', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-04-17', '1222', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-04-19', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-04-21', '1684', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-04-21', '1684', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-04-22', '1218', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-04-22', '1218', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-04-23', '1345', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-04-23', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-04-26', '1031', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-04-26', '1031', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-04-26', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-04-27', '2262', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-04-27', '2318', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-05-01', '2351', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-05-04', '1048', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-05-05', '1380', 'JFK', 'STT'),\n",
       "  ('AA', '2015-05-05', '1380', 'STT', 'JFK'),\n",
       "  ('AA', '2015-05-06', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-05-08', '207', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-05-09', '2315', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-05-09', '2315', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-05-11', '2254', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-05-12', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-05-12', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-05-13', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-05-13', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-05-15', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-05-15', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-05-19', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-05-19', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-05-25', '1380', 'JFK', 'STT'),\n",
       "  ('AA', '2015-05-25', '1380', 'STT', 'JFK'),\n",
       "  ('AA', '2015-05-27', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-05-30', '2254', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-06-01', '1380', 'JFK', 'STT'),\n",
       "  ('AA', '2015-06-01', '1380', 'STT', 'JFK'),\n",
       "  ('AA', '2015-06-03', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-06-04', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-06-04', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-06-06', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-06-07', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-06-07', '1334', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-06-07', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-06-07', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-06-08', '1695', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-06-17', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-06-18', '2315', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-06-18', '2315', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-06-20', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-06-23', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-06-26', '1334', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-06-27', '1695', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-06-28', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-06-28', '2254', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-06-29', '936', 'JFK', 'STT'),\n",
       "  ('AA', '2015-06-29', '936', 'STT', 'JFK'),\n",
       "  ('AA', '2015-07-02', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-07-02', '2315', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-07-02', '2315', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-07-05', '1254', 'MIA', 'SJU'),\n",
       "  ('AA', '2015-07-05', '1254', 'SJU', 'MIA'),\n",
       "  ('AA', '2015-07-05', '2254', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-07-06', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-07-06', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-07-06', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-07-08', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-07-09', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-07-09', '2275', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-07-09', '2426', 'DFW', 'TUL'),\n",
       "  ('AA', '2015-07-20', '1314', 'TUL', 'DFW'),\n",
       "  ('AA', '2015-07-21', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-07-24', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-07-27', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-07-27', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-07-28', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-07-29', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-08-03', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-08-03', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-08-04', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-08-06', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-08-08', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-08-09', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-08-10', '936', 'JFK', 'STT'),\n",
       "  ('AA', '2015-08-10', '936', 'STT', 'JFK'),\n",
       "  ('AA', '2015-08-11', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-08-11', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-08-12', '936', 'JFK', 'STT'),\n",
       "  ('AA', '2015-08-12', '936', 'STT', 'JFK'),\n",
       "  ('AA', '2015-08-15', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-08-15', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-08-15', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-08-17', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-08-21', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-08-21', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-08-23', '1553', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-08-23', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-08-26', '1357', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-08-26', '1357', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-08-27', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-08-31', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-09-01', '2243', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-09-04', '2379', 'MIA', 'STT'),\n",
       "  ('AA', '2015-09-05', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-09-05', '1350', 'STT', 'MIA'),\n",
       "  ('AA', '2015-09-05', '1553', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-09-06', '1334', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-09-07', '296', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-09-07', '64', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-09-08', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-09-09', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-09-11', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-09-11', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-09-13', '1553', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-09-15', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-09-19', '943', 'MIA', 'STT'),\n",
       "  ('AA', '2015-09-19', '943', 'STT', 'MIA'),\n",
       "  ('AA', '2015-09-21', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-09-23', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-09-25', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-09-27', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-09-28', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-09-28', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-09-29', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-10-02', '312', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-10-03', '200', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-10-03', '2379', 'MIA', 'STT'),\n",
       "  ('AA', '2015-10-04', '1172', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-10-04', '1350', 'STT', 'MIA'),\n",
       "  ('AA', '2015-10-04', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-10-07', '2427', 'MIA', 'DFW'),\n",
       "  ('AA', '2015-10-09', '1553', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-10-09', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-10-15', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-10-16', '2379', 'MIA', 'STT'),\n",
       "  ('AA', '2015-10-17', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-10-17', '1350', 'STT', 'MIA'),\n",
       "  ('AA', '2015-10-17', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-10-18', '943', 'MIA', 'STT'),\n",
       "  ('AA', '2015-10-18', '943', 'STT', 'MIA'),\n",
       "  ('AA', '2015-10-20', '1553', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-10-20', '206', 'DFW', 'MIA'),\n",
       "  ('AA', '2015-10-24', '2379', 'MIA', 'STT'),\n",
       "  ('AA', '2015-10-24', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-10-25', '1086', 'BOS', 'MIA'),\n",
       "  ('AA', '2015-10-25', '1350', 'STT', 'MIA'),\n",
       "  ('AA', '2015-10-25', '29', 'MIA', 'BOS'),\n",
       "  ('AA', '2015-11-03', '1510', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-11-04', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-11-26', '2489', 'DFW', 'LAX'),\n",
       "  ('AA', '2015-11-27', '252', 'LAX', 'MIA'),\n",
       "  ('AA', '2015-11-28', '1334', 'MIA', 'MCO'),\n",
       "  ('AA', '2015-11-28', '2204', 'MCO', 'MIA'),\n",
       "  ('AA', '2015-12-01', '1406', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-12-02', '937', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-12-02', '937', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-12-14', '937', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-12-14', '937', 'SJU', 'JFK'),\n",
       "  ('AA', '2015-12-17', '196', 'JFK', 'STT'),\n",
       "  ('AA', '2015-12-17', '196', 'STT', 'JFK'),\n",
       "  ('AA', '2015-12-19', '2382', 'JFK', 'MIA'),\n",
       "  ('AA', '2015-12-30', '1406', 'MIA', 'JFK'),\n",
       "  ('AA', '2015-12-31', '937', 'JFK', 'SJU'),\n",
       "  ('AA', '2015-12-31', '937', 'SJU', 'JFK')],\n",
       " 'TailNum': 'N5FNAA'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter down to the fields we need to identify and link to a flight\n",
    "flights = on_time_dataframe.rdd.map(lambda x: \n",
    "  (x.Carrier, x.FlightDate, x.FlightNum, x.Origin, x.Dest, x.TailNum)\n",
    "  )\n",
    "\n",
    "# Group flights by tail number, sorted by date, then flight number, then origin/dest\n",
    "flights_per_airplane = flights\\\n",
    "  .map(lambda nameTuple: (nameTuple[5], [nameTuple[0:5]]))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .map(lambda tuple:\n",
    "      {\n",
    "        'TailNum': tuple[0], \n",
    "        'Flights': sorted(tuple[1], key=lambda x: (x[1], x[2], x[3], x[4]))\n",
    "      }\n",
    "    )\n",
    "\n",
    "\n",
    "# Save to Mongo\n",
    "#import pymongo_spark\n",
    "#pymongo_spark.activate()\n",
    "flights_per_airplane.saveToMongoDB('mongodb://localhost:27017/agile_data_science.flights_per_airplane')\n",
    "flights_per_airplane.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crisliu/agile/spark/python/pyspark/sql/session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "# Dump the unneeded fields\n",
    "tail_numbers = on_time_dataframe.rdd.map(lambda x: x.TailNum)\n",
    "tail_numbers = tail_numbers.filter(lambda x: x != '')\n",
    "\n",
    "# distinct() gets us unique tail numbers\n",
    "unique_tail_numbers = tail_numbers.distinct()\n",
    "\n",
    "# Store as JSON objects via a dataframe. Repartition to 1 to get 1 json file.\n",
    "unique_records = unique_tail_numbers.map(lambda x: {'TailNum': x})\n",
    "unique_records.toDF().repartition(1).write.json(\"../data/tail_numbers.json\")\n",
    "\n",
    "# Now from bash: ls data/tail_numbers.json/part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ../data/tail_numbers.json/part* ../data/tail_numbers.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TailNum': 'N3KJAA'},\n",
       " {'TailNum': 'N632SW'},\n",
       " {'TailNum': 'N537JB'},\n",
       " {'TailNum': 'N920DE'},\n",
       " {'TailNum': 'N200UU'},\n",
       " {'TailNum': 'N138SY'},\n",
       " {'TailNum': 'N7740A'},\n",
       " {'TailNum': 'N14568'},\n",
       " {'TailNum': 'N81449'},\n",
       " {'TailNum': 'N5ESAA'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_records.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: string, Quarter: string, Month: string, DayofMonth: string, DayOfWeek: string, FlightDate: string, Carrier: string, TailNum: string, FlightNum: string, Origin: string, OriginCityName: string, OriginState: string, Dest: string, DestCityName: string, DestState: string, DepTime: string, DepDelay: float, DepDelayMinutes: int, TaxiOut: float, TaxiIn: float, WheelsOff: string, WheelsOn: string, ArrTime: string, ArrDelay: float, ArrDelayMinutes: float, Cancelled: int, Diverted: int, ActualElapsedTime: float, AirTime: float, Flights: int, Distance: float, CarrierDelay: float, WeatherDelay: float, NASDelay: float, SecurityDelay: float, LateAircraftDelay: float, CRSDepTime: string, CRSArrTime: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_time_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "\n",
    "carrier_airplane = spark.sql(\n",
    "  \"SELECT DISTINCT Carrier, TailNum FROM on_time_performance\"\n",
    "  )\n",
    "\n",
    "# Now we need to store a sorted group for each Carrier, along with a fleet count\n",
    "airplanes_per_carrier = carrier_airplane.rdd\\\n",
    "  .map(lambda nameTuple: (nameTuple[0], [nameTuple[1]]))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .map(lambda tuple:\n",
    "      {\n",
    "        'Carrier': tuple[0], \n",
    "        'TailNumbers': sorted(\n",
    "          filter(\n",
    "            lambda x: x is not None and x != '', tuple[1] # empty string tail numbers were getting through\n",
    "            )\n",
    "          ),\n",
    "        'FleetCount': len(tuple[1])\n",
    "      }\n",
    "    )\n",
    "airplanes_per_carrier.count() # 14\n",
    "\n",
    "# Save to Mongo in the airplanes_per_carrier relation\n",
    "airplanes_per_carrier.saveToMongoDB(\n",
    "  'mongodb://localhost:27017/agile_data_science.airplanes_per_carrier'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+----+----+--------------+--------------+------+\n",
      "| ID|                Name|Alias|IATA|ICAO|      CallSign|       Country|Active|\n",
      "+---+--------------------+-----+----+----+--------------+--------------+------+\n",
      "| -1|             Unknown| null|   -| N/A|          null|          null|     Y|\n",
      "|  1|      Private flight| null|   -| N/A|          null|          null|     Y|\n",
      "|  2|         135 Airways| null|null| GNL|       GENERAL| United States|     N|\n",
      "|  3|       1Time Airline| null|  1T| RNX|       NEXTIME|  South Africa|     Y|\n",
      "|  4|2 Sqn No 1 Elemen...| null|null| WYT|          null|United Kingdom|     N|\n",
      "|  5|     213 Flight Unit| null|null| TFU|          null|        Russia|     N|\n",
      "|  6|223 Flight Unit S...| null|null| CHD|CHKALOVSK-AVIA|        Russia|     N|\n",
      "|  7|   224th Flight Unit| null|null| TTF|    CARGO UNIT|        Russia|     N|\n",
      "|  8|         247 Jet Ltd| null|null| TWF|  CLOUD RUNNER|United Kingdom|     N|\n",
      "|  9|         3D Aviation| null|null| SEC|       SECUREX| United States|     N|\n",
      "| 10|         40-Mile Air| null|  Q5| MLA|      MILE-AIR| United States|     Y|\n",
      "| 11|              4D Air| null|null| QRT|       QUARTET|      Thailand|     N|\n",
      "| 12|611897 Alberta Li...| null|null| THD|         DONUT|        Canada|     N|\n",
      "| 13|    Ansett Australia| null|  AN| AAA|        ANSETT|     Australia|     Y|\n",
      "| 14|Abacus International| null|  1B|null|          null|     Singapore|     Y|\n",
      "| 15|     Abelag Aviation| null|  W9| AAB|           ABG|       Belgium|     N|\n",
      "| 16|      Army Air Corps| null|null| AAC|       ARMYAIR|United Kingdom|     N|\n",
      "| 17|Aero Aviation Cen...| null|null| AAD|       SUNRISE|        Canada|     N|\n",
      "| 18|Aero Servicios Ej...| null|null| SII|        ASEISA|        Mexico|     N|\n",
      "| 19|         Aero Biniza| null|null| BZS|        BINIZA|        Mexico|     N|\n",
      "+---+--------------------+-----+----+----+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---------------+-----+----+----+--------+-------------+------+\n",
      "|  ID|           Name|Alias|IATA|ICAO|CallSign|      Country|Active|\n",
      "+----+---------------+-----+----+----+--------+-------------+------+\n",
      "|2009|Delta Air Lines| null|  DL| DAL|   DELTA|United States|     Y|\n",
      "+----+---------------+-----+----+----+--------+-------------+------+\n",
      "\n",
      "+--------------------+-----------+\n",
      "|                Name|CarrierCode|\n",
      "+--------------------+-----------+\n",
      "|     United Airlines|         UA|\n",
      "|     Spirit Airlines|         NK|\n",
      "|   American Airlines|         AA|\n",
      "|Atlantic Southeas...|         EV|\n",
      "|     JetBlue Airways|         B6|\n",
      "|     Delta Air Lines|         DL|\n",
      "|             SkyWest|         OO|\n",
      "|   Frontier Airlines|         F9|\n",
      "|          US Airways|         US|\n",
      "|American Eagle Ai...|         MQ|\n",
      "|   Hawaiian Airlines|         HA|\n",
      "|     Alaska Airlines|         AS|\n",
      "|      Virgin America|         VX|\n",
      "|  Southwest Airlines|         WN|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add_name_to_airlines.py\n",
    "import sys, os, re\n",
    "carrier_codes = spark.sql(\n",
    "  \"SELECT DISTINCT Carrier FROM on_time_performance\"\n",
    "  )\n",
    "carrier_codes.collect()\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"ID\", IntegerType(), True),  # \"ArrDelay\":5.0\n",
    "  StructField(\"Name\", StringType(), True),  # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "  StructField(\"Alias\", StringType(), True),  # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "  StructField(\"IATA\", StringType(), True),  # \"Carrier\":\"WN\"\n",
    "  StructField(\"ICAO\", StringType(), True),  # \"DayOfMonth\":31\n",
    "  StructField(\"CallSign\", StringType(), True),  # \"DayOfWeek\":4\n",
    "  StructField(\"Country\", StringType(), True),  # \"DayOfYear\":365\n",
    "  StructField(\"Active\", StringType(), True),  # \"DepDelay\":14.0\n",
    "])\n",
    "\n",
    "airlines = spark.read.format('com.databricks.spark.csv')\\\n",
    "  .options(header='false', nullValue='\\\\N')\\\n",
    "  .schema(schema)\\\n",
    "  .load('../data/airlines.csv')\n",
    "airlines.show()\n",
    "\n",
    "# Is Delta around?\n",
    "airlines.filter(airlines.IATA == 'DL').show()\n",
    "\n",
    "# Drop fields except for C1 as name, C3 as carrier code\n",
    "airlines.registerTempTable(\"airlines\")\n",
    "airlines = spark.sql(\"SELECT Name, IATA AS CarrierCode from airlines\")\n",
    "\n",
    "# Join our 14 carrier codes to the airliens table to get our set of airlines\n",
    "our_airlines = carrier_codes.join(airlines, carrier_codes.Carrier == airlines.CarrierCode)\n",
    "our_airlines = our_airlines.select('Name', 'CarrierCode')\n",
    "our_airlines.show()\n",
    "\n",
    "# Store as JSON objects via a dataframe. Repartition to 1 to get 1 json file.\n",
    "our_airlines.repartition(1).write.mode(\"overwrite\").json(\"../data/our_airlines.json\")\n",
    "\n",
    "os.system(\"cp ../data/our_airlines.json/part* ../data/our_airlines.jsonl\")\n",
    "\n",
    "#wikidata = spark.read.json('data/wikidata-20160404-all.json.bz2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: string, Quarter: string, Month: string, DayofMonth: string, DayOfWeek: string, FlightDate: string, Carrier: string, TailNum: string, FlightNum: string, Origin: string, OriginCityName: string, OriginState: string, Dest: string, DestCityName: string, DestState: string, DepTime: string, DepDelay: float, DepDelayMinutes: int, TaxiOut: float, TaxiIn: float, WheelsOff: string, WheelsOn: string, ArrTime: string, ArrDelay: float, ArrDelayMinutes: float, Cancelled: int, Diverted: int, ActualElapsedTime: float, AirTime: float, Flights: int, Distance: float, CarrierDelay: float, WeatherDelay: float, NASDelay: float, SecurityDelay: float, LateAircraftDelay: float, CRSDepTime: string, CRSArrTime: string]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_time_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------+--------------------+--------+-----------+--------------------+-------------+-------------+\n",
      "|TailNum|engine_manufacturer|engine_model|        manufacturer|mfr_year|      model|               owner|  owner_state|serial_number|\n",
      "+-------+-------------------+------------+--------------------+--------+-----------+--------------------+-------------+-------------+\n",
      "| N933EV|                 GE| CF34 SERIES|      BOMBARDIER INC|    2005|CL-600-2B19| DELTA AIR LINES INC|      GEORGIA|         8022|\n",
      "| N917WN|           CFM INTL|  CFM56-7B24|              BOEING|    2008|    737-7H4|SOUTHWEST AIRLINE...|        TEXAS|        36624|\n",
      "| N438WN|          CFM INTL.|CFM56 SERIES|              BOEING|    2003|    737-7H4|SOUTHWEST AIRLINE...|        TEXAS|        29833|\n",
      "| N283VA|           CFM INTL| CFM56-5B4/3|              AIRBUS|    2015|   A320-214|  VIRGIN AMERICA INC|   CALIFORNIA|         6787|\n",
      "| N473UA|           IVCHENKO| AL-25SERIES|    AIRBUS INDUSTRIE|    2001|   A320-232| UNITED AIRLINES INC|     ILLINOIS|         1469|\n",
      "| N637JB|                IAE|    V2527-A5|              AIRBUS|    2006|   A320-232|JETBLUE AIRWAYS CORP|     NEW YORK|         2781|\n",
      "| N286AY|         ROLLS-ROYC|   RR772B-60|              AIRBUS|    None|   A330-243|AMERICAN AIRLINES...|        TEXAS|         1415|\n",
      "| N422UA|                IAE| V2500SERIES|    AIRBUS INDUSTRIE|    1995|   A320-232| U S BANK NA TRUSTEE|MASSACHUSETTS|          503|\n",
      "| N641VA|           CFM INTL|   CFM56-5B4|              AIRBUS|    2008|   A320-214|WELLS FARGO BANK ...|         UTAH|         3656|\n",
      "| N825NW|                 GE|  CF6-80E1A4|              AIRBUS|    2015|   A330-302|  US BANK NA TRUSTEE|  CONNECTICUT|         1679|\n",
      "| N957WN|           CFM INTL|CFM56-7B24/3|              BOEING|    2011|    737-7H4|SOUTHWEST AIRLINE...|        TEXAS|        41528|\n",
      "| N8311Q|           CFM INTL| CFM56-7B26E|              BOEING|    2012|    737-8H4|SOUTHWEST AIRLINE...|        TEXAS|        38808|\n",
      "| N931EV|                 GE| CF34 SERIES|      BOMBARDIER INC|    2005|CL-600-2B19| DELTA AIR LINES INC|      GEORGIA|         8015|\n",
      "| N47414|           CFM INTL|  CFM56-7B26|              BOEING|    2008|  737-924ER| UNITED AIRLINES INC|     ILLINOIS|        32827|\n",
      "| N925DL|              P & W| JT8D SERIES|MCDONNELL DOUGLAS...|    1988|      MD-88|WELLS FARGO BANK ...|         UTAH|        49712|\n",
      "| N13550|         ROLLS-ROYC|  AE 3007A1P|             EMBRAER|    2002|  EMB-145LR|WELLS FARGO BANK ...|         UTAH|       145575|\n",
      "| N13750|          CFM INTL.|CFM56 SERIES|              BOEING|    1999|    737-724|WELLS FARGO BANK ...|         UTAH|        28941|\n",
      "| N79521|           CFM INTL|CFM56-7B26/3|              BOEING|    2010|    737-824| UNITED AIRLINES INC|     ILLINOIS|        31662|\n",
      "| N487WN|          CFM INTL.|CFM56 SERIES|              BOEING|    2004|    737-7H4|SOUTHWEST AIRLINE...|        TEXAS|        33854|\n",
      "| N378AA|         CONT MOTOR|   0-300 SER|              CESSNA|    1963|       172E|   UNDERWOOD BRUCE D|    WISCONSIN|     17250969|\n",
      "+-------+-------------------+------------+--------------------+--------+-----------+--------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+\n",
      "|TailNum|\n",
      "+-------+\n",
      "| N3KJAA|\n",
      "| N632SW|\n",
      "| N537JB|\n",
      "| N920DE|\n",
      "| N200UU|\n",
      "| N138SY|\n",
      "| N7740A|\n",
      "| N14568|\n",
      "| N81449|\n",
      "| N5ESAA|\n",
      "| N378NW|\n",
      "| N946WN|\n",
      "| N913JB|\n",
      "| N548CA|\n",
      "| N651AW|\n",
      "| N380DA|\n",
      "| N7825A|\n",
      "| N15986|\n",
      "| N3JRAA|\n",
      "| N791UA|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------------------+-------------+--------------------+--------+---------------+--------------------+-------------+-------------+\n",
      "|TailNum|engine_manufacturer| engine_model|        manufacturer|mfr_year|          model|               owner|  owner_state|serial_number|\n",
      "+-------+-------------------+-------------+--------------------+--------+---------------+--------------------+-------------+-------------+\n",
      "| N632SW|          CFM INTL.| CFM56 SERIES|              BOEING|    1996|        737-3H4|SOUTHWEST AIRLINE...|        TEXAS|        27707|\n",
      "| N537JB|         ROLLS-ROYC|  250-C300/A1|ROBINSON HELICOPT...|    2012|            R66|          JM AIR LLC|     VIRGINIA|         0257|\n",
      "| N920DE|              P & W|  JT8D SERIES|MCDONNELL DOUGLAS...|    1993|          MD-88| DELTA AIR LINES INC|      GEORGIA|        53423|\n",
      "| N200UU|         ROLLS-ROYC|RB.211 SERIES|              BOEING|    1995|        757-2B7|AMERICAN AIRLINES...|        TEXAS|        27809|\n",
      "| N138SY|                 GE|     CF34-8E5|         EMBRAER S A|    2015| ERJ 170-200 LR|SKYWEST AIRLINES INC|         UTAH|     17000466|\n",
      "| N7740A|           CFM INTL|   CFM56-7B22|              BOEING|    2007|        737-7BD|SOUTHWEST AIRLINE...|        TEXAS|        33927|\n",
      "| N14568|         ROLLS-ROYC|   AE 3007A1P|             EMBRAER|    2002|      EMB-145LR|WELLS FARGO BANK ...|         UTAH|       145628|\n",
      "| N81449|           CFM INTL|  CFM56-7B26E|              BOEING|    2012|      737-924ER| UNITED AIRLINES INC|     ILLINOIS|        31651|\n",
      "| N378NW|          CFM INTL.| CFM56 SERIES|              AIRBUS|    2003|       A320-211| DELTA AIR LINES INC|      GEORGIA|         2092|\n",
      "| N946WN|           CFM INTL| CFM56-7B24/3|              BOEING|    2010|        737-7H4|SOUTHWEST AIRLINE...|        TEXAS|        36918|\n",
      "| N913JB|                IAE|     V2533-A5|              AIRBUS|    2013|       A321-231|JETBLUE AIRWAYS CORP|     NEW YORK|         5909|\n",
      "| N548CA|                 GE|     CF34-8C5|      BOMBARDIER INC|    2008|    CL-600-2D24| DELTA AIR LINES INC|      GEORGIA|        15159|\n",
      "| N651AW|            Unknown|      Unknown|    AIRBUS INDUSTRIE|    1998|       A320-232| U S BANK NA TRUSTEE|MASSACHUSETTS|          866|\n",
      "| N380DA|          CFM INTL.| CFM56 SERIES|              BOEING|    1999|        737-832| DELTA AIR LINES INC|      GEORGIA|        30266|\n",
      "| N7825A|           CFM INTL|   CFM56-7B24|              BOEING|    2003|        737-7CT|SOUTHWEST AIRLINE...|        TEXAS|        32750|\n",
      "| N15986|            ALLISON|     AE 3007A|             EMBRAER|    2000|      EMB-145LR| UNITED AIRLINES INC|     ILLINOIS|       145254|\n",
      "| N791UA|                P&W|   PW4000 SER|              BOEING|    1997|        777-222| UNITED AIRLINES INC|     ILLINOIS|        26933|\n",
      "| N925EV|                 GE|  CF34 SERIES|      BOMBARDIER INC|    2003|    CL-600-2B19|EXPRESSJET AIRLIN...|      GEORGIA|         7831|\n",
      "| N860NW|              P & W|      PW4168A|              AIRBUS|    2006|       A330-223| DELTA AIR LINES INC|      GEORGIA|         0778|\n",
      "| N274JB|                 GE|    CF34-10E6|             EMBRAER|    2007|ERJ 190-100 IGW|WELLS FARGO BANK ...|         UTAH|     19000082|\n",
      "+-------+-------------------+-------------+--------------------+--------+---------------+--------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the FAA N-Number inquiry records\n",
    "faa_tail_number_inquiry = spark.read.json('../data/faa_tail_number_inquiry.jsonl')\n",
    "faa_tail_number_inquiry.show()\n",
    "# Count the records\n",
    "faa_tail_number_inquiry.count()\n",
    "# Load our unique tail numbers\n",
    "unique_tail_numbers = spark.read.json('../data/tail_numbers.jsonl')\n",
    "unique_tail_numbers.show()\n",
    "# Join tail numbers to our inquries\n",
    "tail_num_plus_inquiry = unique_tail_numbers.join(\n",
    "  faa_tail_number_inquiry,\n",
    "  unique_tail_numbers.TailNum == faa_tail_number_inquiry.TailNum,\n",
    ")\n",
    "tail_num_plus_inquiry = tail_num_plus_inquiry.drop(unique_tail_numbers.TailNum)\n",
    "tail_num_plus_inquiry.show()\n",
    "# Dump extra field and store tail_numbers plus inquiry\n",
    "tail_num_plus_inquiry.registerTempTable(\"tail_num_plus_inquiry\")\n",
    "airplanes = spark.sql(\"\"\"SELECT\n",
    "  TailNum AS TailNum,\n",
    "  engine_manufacturer AS EngineManufacturer,\n",
    "  engine_model AS EngineModel,\n",
    "  manufacturer AS Manufacturer,\n",
    "  mfr_year AS ManufacturerYear,\n",
    "  model AS Model,\n",
    "  owner AS Owner,\n",
    "  owner_state AS OwnerState,\n",
    "  serial_number AS SerialNumber\n",
    "FROM\n",
    "  tail_num_plus_inquiry\"\"\")\n",
    "airplanes.repartition(1).write.json('../data/airplanes.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze_airplane.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Manufacturer|Total|\n",
      "+--------------------+-----+\n",
      "|              BOEING| 1953|\n",
      "|              AIRBUS|  519|\n",
      "|      BOMBARDIER INC|  428|\n",
      "|    AIRBUS INDUSTRIE|  426|\n",
      "|             EMBRAER|  330|\n",
      "|   MCDONNELL DOUGLAS|  116|\n",
      "|MCDONNELL DOUGLAS...|  103|\n",
      "|         EMBRAER S A|   43|\n",
      "|MCDONNELL DOUGLAS...|   14|\n",
      "|              CESSNA|    9|\n",
      "|            CANADAIR|    8|\n",
      "|               BEECH|    6|\n",
      "|GULFSTREAM AEROSPACE|    5|\n",
      "|               PIPER|    5|\n",
      "|  CIRRUS DESIGN CORP|    3|\n",
      "|RAYTHEON AIRCRAFT...|    2|\n",
      "|DIAMOND AIRCRAFT ...|    2|\n",
      "|             DOUGLAS|    1|\n",
      "|                 DJI|    1|\n",
      "|          AGUSTA SPA|    1|\n",
      "|              SOCATA|    1|\n",
      "|ROBINSON HELICOPT...|    1|\n",
      "|     LAMBERT RICHARD|    1|\n",
      "|        GROSS ROBERT|    1|\n",
      "|                BELL|    1|\n",
      "|          MARZ BARRY|    1|\n",
      "|        KILDALL GARY|    1|\n",
      "|         PAIR MIKE E|    1|\n",
      "|         LEARJET INC|    1|\n",
      "|  AVIAT AIRCRAFT INC|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n",
      "Total airplanes: 3988\n",
      "+--------------------+-----+---------------+\n",
      "|        Manufacturer|Total|PercentageTotal|\n",
      "+--------------------+-----+---------------+\n",
      "|              BOEING| 1953|          48.97|\n",
      "|              AIRBUS|  519|          13.01|\n",
      "|      BOMBARDIER INC|  428|          10.73|\n",
      "|    AIRBUS INDUSTRIE|  426|          10.68|\n",
      "|             EMBRAER|  330|           8.27|\n",
      "|   MCDONNELL DOUGLAS|  116|           2.91|\n",
      "|MCDONNELL DOUGLAS...|  103|           2.58|\n",
      "|         EMBRAER S A|   43|           1.08|\n",
      "|MCDONNELL DOUGLAS...|   14|           0.35|\n",
      "|              CESSNA|    9|           0.23|\n",
      "+--------------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airplanes = spark.read.json('../data/airplanes.json')\n",
    "\n",
    "#\n",
    "# Who makes the airplanes in the US commercial fleet, as a %\n",
    "#\n",
    "\n",
    "# How many airplanes are made by each manufacturer?\n",
    "airplanes.registerTempTable(\"airplanes\")\n",
    "manufacturer_counts = spark.sql(\"\"\"SELECT\n",
    "  Manufacturer,\n",
    "  COUNT(*) AS Total\n",
    "FROM\n",
    "  airplanes\n",
    "GROUP BY\n",
    "  Manufacturer\n",
    "ORDER BY\n",
    "  Total DESC\"\"\"\n",
    ")\n",
    "manufacturer_counts.show(30) # show top 30\n",
    "\n",
    "# How many airplanes total?\n",
    "total_airplanes = spark.sql(\n",
    "  \"\"\"SELECT\n",
    "  COUNT(*) AS OverallTotal\n",
    "  FROM airplanes\"\"\"\n",
    ")\n",
    "print(\"Total airplanes: {}\".format(total_airplanes.collect()[0].OverallTotal))\n",
    "\n",
    "mfr_with_totals = manufacturer_counts.join(total_airplanes)\n",
    "\"\"\"mfr_with_totals = mfr_with_totals.rdd.map(\n",
    "  lambda x: {\n",
    "    'Manufacturer': x.Manufacturer,\n",
    "    'Total': x.Total,\n",
    "    'Percentage': round(\n",
    "      (\n",
    "        float(x.Total)/float(x.OverallTotal)\n",
    "      ) * 100,\n",
    "      2\n",
    "    )\n",
    "  }\n",
    ")\n",
    "mfr_with_totals.toDF().show()\"\"\"\n",
    "#\n",
    "# Same with sub-queries\n",
    "#\n",
    "relative_manufacturer_counts = spark.sql(\"\"\"SELECT\n",
    "  Manufacturer,\n",
    "  COUNT(*) AS Total,\n",
    "  ROUND(\n",
    "    100 * (\n",
    "      COUNT(*)/(SELECT COUNT(*) FROM airplanes)\n",
    "    ),\n",
    "    2\n",
    "  ) AS PercentageTotal\n",
    "FROM\n",
    "  airplanes\n",
    "GROUP BY\n",
    "  Manufacturer\n",
    "ORDER BY\n",
    "  Total DESC, Manufacturer\n",
    "LIMIT 10\"\"\"\n",
    ")\n",
    "relative_manufacturer_counts.show(30) # show top 30\n",
    "\n",
    "#\n",
    "# Now get these things on the web\n",
    "#\n",
    "relative_manufacturer_counts = relative_manufacturer_counts.rdd.map(lambda row: row.asDict())\n",
    "grouped_manufacturer_counts = relative_manufacturer_counts.groupBy(lambda x: 1)\n",
    "\n",
    "# Save to Mongo in the airplanes_per_carrier relation\n",
    "#import pymongo_spark\n",
    "#pymongo_spark.activate()\n",
    "grouped_manufacturer_counts.saveToMongoDB(\n",
    "  'mongodb://localhost:27017/agile_data_science.airplane_manufacturer_totals'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1148.save.\n: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: request [/agile_data_science/airplane/_query] contains unrecognized parameter: [q]\nnull\n\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:475)\n\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:446)\n\tat org.elasticsearch.hadoop.rest.RestClient.delete(RestClient.java:495)\n\tat org.elasticsearch.hadoop.rest.RestRepository.delete(RestRepository.java:422)\n\tat org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:481)\n\tat org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:76)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:426)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-c083b4b353df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#airplanes.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mairplanes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.elasticsearch.spark.sql\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.resource\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"agile_data_science/airplane\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/agile/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1148.save.\n: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: request [/agile_data_science/airplane/_query] contains unrecognized parameter: [q]\nnull\n\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:475)\n\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:446)\n\tat org.elasticsearch.hadoop.rest.RestClient.delete(RestClient.java:495)\n\tat org.elasticsearch.hadoop.rest.RestRepository.delete(RestRepository.java:422)\n\tat org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:481)\n\tat org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:76)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:426)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "airplanes = spark.read.json(\"../data/airplanes.json\")\n",
    "#airplanes.show()\n",
    "\n",
    "airplanes.write.format(\"org.elasticsearch.spark.sql\")\\\n",
    ".option(\"es.resource\",\"agile_data_science/airplane\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 206.0 failed 1 times, most recent failure: Lost task 0.0 in stage 206.0 (TID 4629, localhost, executor driver): org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: request [/agile_data_science/_search] contains unrecognized parameter: [_local]\n{\"slice\":{\"id\":4,\"max\":58},\"query\":{\"match_all\":{}}}\n\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:475)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:433)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)\n\tat org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)\n\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1353)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1326)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:203)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:582)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: request [/agile_data_science/_search] contains unrecognized parameter: [_local]\n{\"slice\":{\"id\":4,\"max\":58},\"query\":{\"match_all\":{}}}\n\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:475)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:433)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)\n\tat org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)\n\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-e102c311bd82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"es.resource\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"agile_data_science\"\u001b[0m\u001b[0;34m}\u001b[0m   \u001b[0;31m# assume Elasticsearch is running on localhost defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewAPIHadoopRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.elasticsearch.hadoop.mr.EsInputFormat\"\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;34m\"org.apache.hadoop.io.NullWritable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.elasticsearch.hadoop.mr.LinkedMapWritable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# the result is a MapWritable that is converted to a Python dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mnewAPIHadoopRDD\u001b[0;34m(self, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[1;32m    673\u001b[0m         jrdd = self._jvm.PythonRDD.newAPIHadoopRDD(self._jsc, inputFormatClass, keyClass,\n\u001b[1;32m    674\u001b[0m                                                    \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                                                    jconf, batchSize)\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/agile/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 206.0 failed 1 times, most recent failure: Lost task 0.0 in stage 206.0 (TID 4629, localhost, executor driver): org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: request [/agile_data_science/_search] contains unrecognized parameter: [_local]\n{\"slice\":{\"id\":4,\"max\":58},\"query\":{\"match_all\":{}}}\n\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:475)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:433)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)\n\tat org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)\n\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1353)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1326)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:203)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:582)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: request [/agile_data_science/_search] contains unrecognized parameter: [_local]\n{\"slice\":{\"id\":4,\"max\":58},\"query\":{\"match_all\":{}}}\n\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:475)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:433)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)\n\tat org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)\n\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1353)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "conf2 = {\"es.resource\" : \"agile_data_science\"}   # assume Elasticsearch is running on localhost defaults\n",
    "rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\",\\\n",
    "    \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=conf2)\n",
    "rdd.first()         # the result is a MapWritable that is converted to a Python dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"Debugging Prediction Problems\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "  sc and spark\n",
    "except NameError as e:\n",
    "  import findspark\n",
    "  findspark.init()\n",
    "  import pyspark\n",
    "  import pyspark.sql\n",
    "\n",
    "  sc = pyspark.SparkContext()\n",
    "  spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Objects from CSV\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 'Relato', 'name': 'Russell Jurney', 'title': 'CEO'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "  parts = line.split(\",\")\n",
    "  record = {\n",
    "    \"name\": parts[0],\n",
    "    \"company\": parts[1],\n",
    "    \"title\": parts[2]\n",
    "  }\n",
    "  return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'job_count': 1, 'name': 'Florian Liebert'},\n",
       " {'job_count': 1, 'name': 'Don Brown'},\n",
       " {'job_count': 2, 'name': 'Russell Jurney'},\n",
       " {'job_count': 1, 'name': 'Donald Trump'},\n",
       " {'job_count': 1, 'name': 'Steve Jobs'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.first()\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map vs FlatMap\n",
    "\n",
    "Understanding the difference between the map and flatmap operators..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Russell Jurney', 'Relato', 'CEO'], ['Florian Liebert', 'Mesosphere', 'CEO'], ['Don Brown', 'Rocana', 'CIO'], ['Steve Jobs', 'Apple', 'CEO'], ['Donald Trump', 'The Trump Organization', 'CEO'], ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Russell Jurney',\n",
       " 'Relato',\n",
       " 'CEO',\n",
       " 'Florian Liebert',\n",
       " 'Mesosphere',\n",
       " 'CEO',\n",
       " 'Don Brown',\n",
       " 'Rocana',\n",
       " 'CIO',\n",
       " 'Steve Jobs',\n",
       " 'Apple',\n",
       " 'CEO',\n",
       " 'Donald Trump',\n",
       " 'The Trump Organization',\n",
       " 'CEO',\n",
       " 'Russell Jurney',\n",
       " 'Data Syndrome',\n",
       " 'Principal Consultant']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "print(words_by_line.collect())\n",
    "\n",
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Rows\n",
    "\n",
    "Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "  parts = line.split(\",\")\n",
    "  row = Row(\n",
    "    name=parts[0],\n",
    "    company=parts[1],\n",
    "    title=parts[2]\n",
    "  )\n",
    "  return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `DataFrames` from `RDDs`\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Donald Trump', total=1),\n",
       " Row(name='Florian Liebert', total=1),\n",
       " Row(name='Don Brown', total=1),\n",
       " Row(name='Russell Jurney', total=2),\n",
       " Row(name='Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Inspecting Parquet Files\n",
    "\n",
    "Using the `SparkSession` to load files as `DataFrames` and inspecting their contents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|FlightDate|TailNum|Origin|Dest|Carrier|DepDelay|ArrDelay|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|2015-01-01| N001AA|   DFW| MEM|     AA|    -3.0|    -6.0|\n",
      "|2015-01-01| N001AA|   MEM| DFW|     AA|    -4.0|    -9.0|\n",
      "|2015-01-01| N002AA|   ORD| DFW|     AA|     0.0|    26.0|\n",
      "|2015-01-01| N003AA|   DFW| ATL|     AA|   100.0|   112.0|\n",
      "|2015-01-01| N003AA|   DFW| HDN|     AA|    78.0|    78.0|\n",
      "|2015-01-01| N003AA|   HDN| DFW|     AA|   332.0|   336.0|\n",
      "|2015-01-01| N004AA|   JAC| DFW|     AA|    -4.0|    21.0|\n",
      "|2015-01-01| N005AA|   EGE| ORD|     AA|    null|    null|\n",
      "|2015-01-01| N005AA|   ORD| EGE|     AA|    null|    null|\n",
      "|2015-01-01| N005AA|   DFW| ORD|     AA|    null|    null|\n",
      "|2015-01-01| N006AA|   DFW| ATL|     AA|    null|    null|\n",
      "|2015-01-01| N006AA|   ATL| DFW|     AA|    -5.0|     1.0|\n",
      "|2015-01-01| N006AA|   DFW| ATL|     AA|    -4.0|   -11.0|\n",
      "|2015-01-01| N007AA|   DFW| ATL|     AA|    76.0|    86.0|\n",
      "|2015-01-01| N008AA|   ATL| DFW|     AA|    -2.0|    -7.0|\n",
      "|2015-01-01| N008AA|   DFW| ATL|     AA|    -5.0|   -25.0|\n",
      "|2015-01-01| N009AA|   DFW| EGE|     AA|    35.0|    17.0|\n",
      "|2015-01-01| N009AA|   EGE| LAX|     AA|    10.0|   -12.0|\n",
      "|2015-01-01| N010AA|   DFW| SDF|     AA|    null|    null|\n",
      "|2015-01-01| N010AA|   SDF| DFW|     AA|    null|    null|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|FlightDate|TailNum|Origin|Dest|Carrier|DepDelay|ArrDelay|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|2015-01-01| N821AW|   MCO| PHL|     US|     3.0|    -4.0|\n",
      "|2015-01-10| N284WN|   BWI| SAN|     WN|    41.0|    22.0|\n",
      "|2015-01-10| N416WN|   MSP| MDW|     WN|    37.0|    25.0|\n",
      "|2015-01-11| N662JB|   BOS| FLL|     B6|    -1.0|   -20.0|\n",
      "|2015-01-13| N440UA|   ORD| DEN|     UA|    20.0|    25.0|\n",
      "|2015-01-15| N588NK|   FLL| SJU|     NK|    -5.0|   -14.0|\n",
      "|2015-01-16| N685BR|   GTF| SLC|     OO|    -6.0|     6.0|\n",
      "|2015-01-16| N7720F|   SNA| PHX|     WN|    18.0|     2.0|\n",
      "|2015-01-16| N7726A|   OAK| SNA|     WN|    -1.0|    -7.0|\n",
      "|2015-01-17| N3CTAA|   SFO| DFW|     AA|    -2.0|   -18.0|\n",
      "|2015-01-18| N980EV|   ATL| FSM|     EV|    -7.0|     2.0|\n",
      "|2015-01-19| N944UW|   DCA| LGA|     US|    -7.0|     0.0|\n",
      "|2015-01-02| N531NK|   FLL| DTW|     NK|    61.0|    50.0|\n",
      "|2015-01-02| N914WN|   DAL| DEN|     WN|     0.0|   -21.0|\n",
      "|2015-01-20| N649MQ|   DFW| GCK|     MQ|     2.0|     3.0|\n",
      "|2015-01-20| N682MQ|   MIA| EYW|     MQ|    -5.0|   -13.0|\n",
      "|2015-01-20| N465SW|   RNO| LAX|     OO|    -6.0|    -6.0|\n",
      "|2015-01-20| N69834|   MCO| ORD|     UA|    -2.0|   -10.0|\n",
      "|2015-01-22| N965DN|   MCI| DTW|     DL|    -3.0|   -27.0|\n",
      "|2015-01-23| N488AA|   DFW| DAY|     AA|    -1.0|    18.0|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the parquet file containing flight delay records\n",
    "on_time_dataframe = spark.read.parquet('../data/on_time_performance.parquet')\n",
    "\n",
    "# Register the data for Spark SQL\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "\n",
    "# Check out the columns\n",
    "on_time_dataframe.columns\n",
    "\n",
    "# Check out some data\n",
    "on_time_dataframe\\\n",
    "  .select(\"FlightDate\", \"TailNum\", \"Origin\", \"Dest\", \"Carrier\", \"DepDelay\", \"ArrDelay\")\\\n",
    "  .show()\n",
    "\n",
    "# Trim the fields and keep the result\n",
    "trimmed_on_time = on_time_dataframe\\\n",
    "  .select(\n",
    "    \"FlightDate\",\n",
    "    \"TailNum\",\n",
    "    \"Origin\",\n",
    "    \"Dest\",\n",
    "    \"Carrier\",\n",
    "    \"DepDelay\",\n",
    "    \"ArrDelay\"\n",
    "  )\n",
    "\n",
    "# Sample 0.01% of the data and show\n",
    "trimmed_on_time.sample(False, 0.0001).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Histograms\n",
    "\n",
    "Using `RDDs` to calculate histograms buckets and values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-82.0,\n",
       "  125.0,\n",
       "  332.0,\n",
       "  539.0,\n",
       "  746.0,\n",
       "  953.0,\n",
       "  1160.0,\n",
       "  1367.0,\n",
       "  1574.0,\n",
       "  1781.0,\n",
       "  1988.0],\n",
       " [11247596, 201786, 12808, 2026, 972, 422, 152, 68, 18, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a histogram of departure delays\n",
    "on_time_dataframe\\\n",
    "  .select(\"DepDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Histograms\n",
    "\n",
    "Using pyplot to visualize histograms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 13 artists>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAFkCAYAAADYL8pXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS42ZmZmVwv8Pdk7rAOD9zSwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1186d2d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot a histogram using pyplot\n",
    "def create_hist(rdd_histogram_data):\n",
    "  \"\"\"Given an RDD.histogram, plot a pyplot histogram\"\"\"\n",
    "  heights = np.array(rdd_histogram_data[1])\n",
    "  full_bins = rdd_histogram_data[0]\n",
    "  mid_point_bins = full_bins[:-1]\n",
    "  widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]\n",
    "  bar = plt.bar(mid_point_bins, heights, width=widths, color='b')\n",
    "  return bar\n",
    "\n",
    "# Compute a histogram of departure delays\n",
    "departure_delay_histogram = on_time_dataframe\\\n",
    "  .select(\"DepDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([-60,-30,-15,-10,-5,0,5,10,15,30,60,90,120,180])\n",
    "\n",
    "create_hist(departure_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Airplanes in the US Fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total airplanes: 4898\n"
     ]
    }
   ],
   "source": [
    "# Dump the unneeded fields\n",
    "tail_numbers = on_time_dataframe.rdd.map(lambda x: x.TailNum)\n",
    "tail_numbers = tail_numbers.filter(lambda x: x != '')\n",
    "\n",
    "# distinct() gets us unique tail numbers\n",
    "unique_tail_numbers = tail_numbers.distinct()\n",
    "\n",
    "# now we need a count() of unique tail numbers\n",
    "airplane_count = unique_tail_numbers.count()\n",
    "print(\"Total airplanes: {}\".format(airplane_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the Total Flights Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Month': '1', 'Year': '2015', 'total_flights': 939936},\n",
       " {'Month': '10', 'Year': '2015', 'total_flights': 972330},\n",
       " {'Month': '11', 'Year': '2015', 'total_flights': 935944},\n",
       " {'Month': '12', 'Year': '2015', 'total_flights': 958460},\n",
       " {'Month': '2', 'Year': '2015', 'total_flights': 858382},\n",
       " {'Month': '3', 'Year': '2015', 'total_flights': 1008624},\n",
       " {'Month': '4', 'Year': '2015', 'total_flights': 970302},\n",
       " {'Month': '5', 'Year': '2015', 'total_flights': 993986},\n",
       " {'Month': '6', 'Year': '2015', 'total_flights': 1007794},\n",
       " {'Month': '7', 'Year': '2015', 'total_flights': 1041436},\n",
       " {'Month': '8', 'Year': '2015', 'total_flights': 1021072},\n",
       " {'Month': '9', 'Year': '2015', 'total_flights': 929892}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use SQL to look at the total flights by month across 2015\n",
    "on_time_dataframe.registerTempTable(\"on_time_dataframe\")\n",
    "total_flights_by_month = spark.sql(\n",
    "  \"\"\"SELECT Month, Year, COUNT(*) AS total_flights\n",
    "  FROM on_time_dataframe\n",
    "  GROUP BY Year, Month\n",
    "  ORDER BY Year, Month\"\"\"\n",
    ")\n",
    "\n",
    "# This map/asDict trick makes the rows print a little prettier. It is optional.\n",
    "flights_chart_data = total_flights_by_month.rdd.map(lambda row: row.asDict())\n",
    "flights_chart_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `RDDs` and Map/Reduce to Prepare a Complex Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flights': [('UA', '2015-09-05', '1132', 'DEN', 'LAX'),\n",
       "  ('UA', '2015-09-05', '1132', 'DEN', 'LAX'),\n",
       "  ('UA', '2015-09-05', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-05', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-05', '1804', 'LAX', 'DEN'),\n",
       "  ('UA', '2015-09-05', '1804', 'LAX', 'DEN'),\n",
       "  ('UA', '2015-09-06', '1977', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-06', '1977', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-08', '1423', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-08', '1423', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-08', '1693', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-08', '1693', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-09', '1614', 'IAH', 'SFO'),\n",
       "  ('UA', '2015-09-09', '1614', 'IAH', 'SFO'),\n",
       "  ('UA', '2015-09-09', '1937', 'SFO', 'IAH'),\n",
       "  ('UA', '2015-09-09', '1937', 'SFO', 'IAH'),\n",
       "  ('UA', '2015-09-10', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-10', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-10', '1631', 'DEN', 'LAX'),\n",
       "  ('UA', '2015-09-10', '1631', 'DEN', 'LAX'),\n",
       "  ('UA', '2015-09-10', '1653', 'LAX', 'DEN'),\n",
       "  ('UA', '2015-09-10', '1653', 'LAX', 'DEN'),\n",
       "  ('UA', '2015-09-10', '1977', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-10', '1977', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-11', '1181', 'SFO', 'IAH'),\n",
       "  ('UA', '2015-09-11', '1181', 'SFO', 'IAH'),\n",
       "  ('UA', '2015-09-11', '1614', 'IAH', 'SFO'),\n",
       "  ('UA', '2015-09-11', '1614', 'IAH', 'SFO'),\n",
       "  ('UA', '2015-09-12', '1715', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-12', '1715', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-09-12', '1977', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-12', '1977', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-09-21', '1126', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-09-21', '1126', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-09-22', '1905', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-09-22', '1905', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-09-24', '1126', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-09-24', '1126', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-09-25', '1051', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-09-25', '1051', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-09-29', '1126', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-09-29', '1126', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-09-30', '1051', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-09-30', '1051', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-10-08', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-10-08', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-10-19', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-10-19', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-10-23', '1405', 'LAX', 'DEN'),\n",
       "  ('UA', '2015-10-23', '1405', 'LAX', 'DEN'),\n",
       "  ('UA', '2015-10-23', '1728', 'DEN', 'LAX'),\n",
       "  ('UA', '2015-10-23', '1728', 'DEN', 'LAX'),\n",
       "  ('UA', '2015-10-29', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-10-29', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-10-31', '1030', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-10-31', '1030', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-10-31', '1877', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-10-31', '1877', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-11-03', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-11-03', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-11-12', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-11-12', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-11-14', '1865', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-11-14', '1865', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-11-14', '328', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-11-14', '328', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-11-19', '1051', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-11-19', '1051', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-11-19', '328', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-11-19', '328', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-11-22', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-11-22', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-11-26', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-11-26', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-11-28', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-11-28', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-12-02', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-12-02', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-12-04', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-12-04', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-12-16', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-12-16', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-12-18', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-12-18', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-12-22', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-12-22', '1963', 'LAX', 'IAH'),\n",
       "  ('UA', '2015-12-24', '2011', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-12-24', '2011', 'DEN', 'IAH'),\n",
       "  ('UA', '2015-12-24', '507', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-12-24', '507', 'IAH', 'DEN'),\n",
       "  ('UA', '2015-12-28', '1169', 'IAH', 'LAX'),\n",
       "  ('UA', '2015-12-28', '1169', 'IAH', 'LAX')],\n",
       " 'TailNum': 'N27957'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter down to the fields we need to identify and link to a flight\n",
    "flights = on_time_dataframe.rdd.map(lambda x: \n",
    "  (x.Carrier, x.FlightDate, x.FlightNum, x.Origin, x.Dest, x.TailNum)\n",
    "  )\n",
    "\n",
    "# Group flights by tail number, sorted by date, then flight number, then origin/dest\n",
    "flights_per_airplane = flights\\\n",
    "  .map(lambda nameTuple: (nameTuple[5], [nameTuple[0:5]]))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .map(lambda tuple:\n",
    "      {\n",
    "        'TailNum': tuple[0], \n",
    "        'Flights': sorted(tuple[1], key=lambda x: (x[1], x[2], x[3], x[4]))\n",
    "      }\n",
    "    )\n",
    "flights_per_airplane.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Late Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_flights = on_time_dataframe.count()\n",
    "\n",
    "# Flights that were late leaving...\n",
    "late_departures = on_time_dataframe.filter(\n",
    "  on_time_dataframe.DepDelayMinutes > 0\n",
    ")\n",
    "total_late_departures = late_departures.count()\n",
    "print(total_late_departures)\n",
    "\n",
    "# Flights that were late arriving...\n",
    "late_arrivals = on_time_dataframe.filter(\n",
    "  on_time_dataframe.ArrDelayMinutes > 0\n",
    ")\n",
    "total_late_arrivals = late_arrivals.count()\n",
    "print(total_late_arrivals)\n",
    "\n",
    "# Get the percentage of flights that are late, rounded to 1 decimal place\n",
    "pct_late = round((total_late_arrivals / (total_flights * 1.0)) * 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Flights with Hero Captains\n",
    "\n",
    "\"Hero Captains\" are those that depart late but make up time in the air and arrive on time or early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flights that left late but made up time to arrive on time...\n",
    "on_time_heros = on_time_dataframe.filter(\n",
    "  (on_time_dataframe.DepDelayMinutes > 0)\n",
    "  &\n",
    "  (on_time_dataframe.ArrDelayMinutes <= 0)\n",
    ")\n",
    "total_on_time_heros = on_time_heros.count()\n",
    "print(total_on_time_heros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total flights:   11,638,158\n",
      "Late departures: 4,251,236\n",
      "Late arrivals:   4,173,792\n",
      "Recoveries:      1,213,804\n",
      "Percentage Late: 35.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Total flights:   {:,}\".format(total_flights))\n",
    "print(\"Late departures: {:,}\".format(total_late_departures))\n",
    "print(\"Late arrivals:   {:,}\".format(total_late_arrivals))\n",
    "print(\"Recoveries:      {:,}\".format(total_on_time_heros))\n",
    "print(\"Percentage Late: {}%\".format(pct_late))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Average Lateness Per Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|AvgDepDelay|AvgArrDelay|\n",
      "+-----------+-----------+\n",
      "|        9.4|        4.4|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the average minutes late departing and arriving\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ROUND(AVG(DepDelay),1) AS AvgDepDelay,\n",
    "  ROUND(AVG(ArrDelay),1) AS AvgArrDelay\n",
    "FROM on_time_performance\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Late Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+--------+-------------+-----------------+\n",
      "|ArrDelayMinutes|WeatherDelay|CarrierDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+---------------+------------+------------+--------+-------------+-----------------+\n",
      "|           25.0|         0.0|        25.0|     0.0|          0.0|              0.0|\n",
      "|          147.0|         0.0|        20.0|     0.0|          0.0|            127.0|\n",
      "|           17.0|         0.0|         0.0|    17.0|          0.0|              0.0|\n",
      "|           18.0|         0.0|         0.0|    18.0|          0.0|              0.0|\n",
      "|           47.0|         0.0|         0.0|    47.0|          0.0|              0.0|\n",
      "|          235.0|         0.0|         0.0|     0.0|          0.0|            235.0|\n",
      "|           23.0|         0.0|        23.0|     0.0|          0.0|              0.0|\n",
      "|           23.0|         0.0|         1.0|    17.0|          0.0|              5.0|\n",
      "|           28.0|         0.0|         0.0|    28.0|          0.0|              0.0|\n",
      "|           51.0|         0.0|        35.0|    11.0|          0.0|              5.0|\n",
      "|           35.0|         0.0|         0.0|    35.0|          0.0|              0.0|\n",
      "|           38.0|         0.0|         0.0|    32.0|          0.0|              6.0|\n",
      "|           40.0|        17.0|         0.0|    11.0|          0.0|             12.0|\n",
      "|           20.0|         0.0|         0.0|    20.0|          0.0|              0.0|\n",
      "|           20.0|         0.0|         0.0|     3.0|          0.0|             17.0|\n",
      "|           69.0|         0.0|         0.0|    13.0|          0.0|             56.0|\n",
      "|           31.0|         0.0|        25.0|     6.0|          0.0|              0.0|\n",
      "|           56.0|         0.0|         9.0|    47.0|          0.0|              0.0|\n",
      "|           87.0|         0.0|        87.0|     0.0|          0.0|              0.0|\n",
      "|           53.0|         0.0|        53.0|     0.0|          0.0|              0.0|\n",
      "+---------------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Why are flights late? Lets look at some delayed flights and the delay causes\n",
    "late_flights = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ArrDelayMinutes,\n",
    "  WeatherDelay,\n",
    "  CarrierDelay,\n",
    "  NASDelay,\n",
    "  SecurityDelay,\n",
    "  LateAircraftDelay\n",
    "FROM\n",
    "  on_time_performance\n",
    "WHERE\n",
    "  WeatherDelay IS NOT NULL\n",
    "  OR\n",
    "  CarrierDelay IS NOT NULL\n",
    "  OR\n",
    "  NASDelay IS NOT NULL\n",
    "  OR\n",
    "  SecurityDelay IS NOT NULL\n",
    "  OR\n",
    "  LateAircraftDelay IS NOT NULL\n",
    "ORDER BY\n",
    "  FlightDate\n",
    "\"\"\")\n",
    "late_flights.sample(False, 0.01).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Why Flights Are Late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-------------+------------------+-----------------------+\n",
      "|pct_weather_delay|pct_carrier_delay|pct_nas_delay|pct_security_delay|pct_late_aircraft_delay|\n",
      "+-----------------+-----------------+-------------+------------------+-----------------------+\n",
      "|              4.5|             29.2|         20.7|               0.1|                   36.1|\n",
      "+-----------------+-----------------+-------------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage contribution to delay for each source\n",
    "total_delays = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ROUND(SUM(WeatherDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_weather_delay,\n",
    "  ROUND(SUM(CarrierDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_carrier_delay,\n",
    "  ROUND(SUM(NASDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_nas_delay,\n",
    "  ROUND(SUM(SecurityDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_security_delay,\n",
    "  ROUND(SUM(LateAircraftDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_late_aircraft_delay\n",
    "FROM on_time_performance\n",
    "\"\"\")\n",
    "total_delays.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Histogram of Weather Delayed Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 5, 10, 15, 30, 60, 120, 240, 480, 720, 1440.0], [10872, 15336, 13272, 32014, 27138, 18884, 9196, 2272, 304, 144])\n"
     ]
    }
   ],
   "source": [
    "# Eyeball the first to define our buckets\n",
    "weather_delay_histogram = on_time_dataframe\\\n",
    "  .select(\"WeatherDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([1, 5, 10, 15, 30, 60, 120, 240, 480, 720, 24*60.0])\n",
    "print(weather_delay_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 8 artists>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqNRZOiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162b1438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_hist(weather_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a Histogram for Visualization by d3.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'on_time_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c637286a0eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Recompute the weather histogram with a filter for on-time flights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m weather_delay_histogram = on_time_dataframe  .filter(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mon_time_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeatherDelay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'on_time_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# Transform the data into something easily consumed by d3\n",
    "def histogram_to_publishable(histogram):\n",
    "  record = {'key': 1, 'data': []}\n",
    "  for label, value in zip(histogram[0], histogram[1]):\n",
    "    record['data'].append(\n",
    "      {\n",
    "        'label': label,\n",
    "        'value': value\n",
    "      }\n",
    "    )\n",
    "  return record\n",
    "\n",
    "# Recompute the weather histogram with a filter for on-time flights\n",
    "weather_delay_histogram = on_time_dataframe\\\n",
    "  .filter(\n",
    "    (on_time_dataframe.WeatherDelay != None)\n",
    "    &\n",
    "    (on_time_dataframe.WeatherDelay > 0)\n",
    "  )\\\n",
    "  .select(\"WeatherDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([0, 15, 30, 60, 120, 240, 480, 720, 24*60.0])\n",
    "print(weather_delay_histogram)\n",
    "\n",
    "record = histogram_to_publishable(weather_delay_histogram)\n",
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Classifier Model to Predict Flight Delays\n",
    "\n",
    "## Loading Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ArrDelay=13.0, CRSArrTime=datetime.datetime(2015, 1, 1, 10, 10), CRSDepTime=datetime.datetime(2015, 1, 1, 7, 30), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=14.0, Dest='DFW', Distance=569.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='1024', Origin='ABQ')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "  StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "  StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "  StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "  StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "  StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "  StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "  StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "  StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "  StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "  StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "  StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "  StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])\n",
    "\n",
    "features = spark.read.json(\n",
    "  \"../data/simple_flight_delay_features.jsonl.bz2\",\n",
    "  schema=schema\n",
    ")\n",
    "features.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check for nulls in features before using Spark ML\n",
    "#\n",
    "null_counts = [(column, features.where(features[column].isNull()).count()) for column in features.columns]\n",
    "cols_with_nulls = filter(lambda x: x[1] > 0, null_counts)\n",
    "print(list(cols_with_nulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Route Column\n",
    "\n",
    "Demonstrating the addition of a feature to our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|Origin|Dest|  Route|\n",
      "+------+----+-------+\n",
      "|   ABQ| DFW|ABQ-DFW|\n",
      "|   ABQ| DFW|ABQ-DFW|\n",
      "|   ABQ| DFW|ABQ-DFW|\n",
      "|   ATL| DFW|ATL-DFW|\n",
      "|   ATL| DFW|ATL-DFW|\n",
      "+------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Add a Route variable to replace FlightNum\n",
    "#\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "features_with_route = features.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    features.Origin,\n",
    "    lit('-'),\n",
    "    features.Dest\n",
    "  )\n",
    ")\n",
    "features_with_route.select(\"Origin\", \"Dest\", \"Route\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizing ArrDelay into ArrDelayBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|    13.0|           2.0|\n",
      "|    17.0|           2.0|\n",
      "|    36.0|           3.0|\n",
      "|   -21.0|           0.0|\n",
      "|   -14.0|           1.0|\n",
      "|    16.0|           2.0|\n",
      "|    -7.0|           1.0|\n",
      "|    13.0|           2.0|\n",
      "|    25.0|           2.0|\n",
      "|    58.0|           3.0|\n",
      "|    14.0|           2.0|\n",
      "|     1.0|           2.0|\n",
      "|   -29.0|           0.0|\n",
      "|   -10.0|           1.0|\n",
      "|    -3.0|           1.0|\n",
      "|    -8.0|           1.0|\n",
      "|    -1.0|           1.0|\n",
      "|   -14.0|           1.0|\n",
      "|   -16.0|           0.0|\n",
      "|    18.0|           2.0|\n",
      "+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Use pysmark.ml.feature.Bucketizer to bucketize ArrDelay\n",
    "#\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\",\n",
    "  outputCol=\"ArrDelayBucket\"\n",
    ")\n",
    "ml_bucketized_features = bucketizer.transform(features_with_route)\n",
    "\n",
    "# Check the buckets out\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Our String Fields into Numeric Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+-------------+----------------+---------------+---------------+------------+----------+-----------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|ArrDelayBucket|Carrier_index|DayOfMonth_index|DayOfWeek_index|DayOfYear_index|Origin_index|Dest_index|Route_index|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+-------------+----------------+---------------+---------------+------------+----------+-----------+\n",
      "|    13.0|2015-01-01 10:10:...|2015-01-01 07:30:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1024|   ABQ|ABQ-DFW|           2.0|          2.0|            25.0|            0.0|          320.0|        53.0|       2.0|      938.0|\n",
      "|    17.0|2015-01-01 02:15:...|2014-12-31 23:25:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1184|   ABQ|ABQ-DFW|           2.0|          2.0|            25.0|            0.0|          320.0|        53.0|       2.0|      938.0|\n",
      "|    36.0|2015-01-01 03:45:...|2015-01-01 01:00:...|     AA|         1|        4|        1|    -2.0| DFW|   569.0|2014-12-31|      336|   ABQ|ABQ-DFW|           3.0|          2.0|            25.0|            0.0|          320.0|        53.0|       2.0|      938.0|\n",
      "|   -21.0|2015-01-01 11:30:...|2015-01-01 09:55:...|     AA|         1|        4|        1|    -1.0| DFW|   731.0|2014-12-31|      125|   ATL|ATL-DFW|           0.0|          2.0|            25.0|            0.0|          320.0|         0.0|       2.0|       37.0|\n",
      "|   -14.0|2015-01-01 02:25:...|2015-01-01 00:55:...|     AA|         1|        4|        1|    -4.0| DFW|   731.0|2014-12-31|     1455|   ATL|ATL-DFW|           1.0|          2.0|            25.0|            0.0|          320.0|         0.0|       2.0|       37.0|\n",
      "|    16.0|2015-01-01 07:15:...|2015-01-01 05:45:...|     AA|         1|        4|        1|    15.0| DFW|   731.0|2014-12-31|     1473|   ATL|ATL-DFW|           2.0|          2.0|            25.0|            0.0|          320.0|         0.0|       2.0|       37.0|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+-------------+----------------+---------------+---------------+------------+----------+-----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Extract features tools in with pyspark.ml.feature\n",
    "#\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Turn category fields into categoric feature vectors, then drop intermediate fields\n",
    "for column in [\"Carrier\", \"DayOfMonth\", \"DayOfWeek\", \"DayOfYear\",\n",
    "               \"Origin\", \"Dest\", \"Route\"]:\n",
    "  string_indexer = StringIndexer(\n",
    "    inputCol=column,\n",
    "    outputCol=column + \"_index\"\n",
    "  )\n",
    "  ml_bucketized_features = string_indexer.fit(ml_bucketized_features)\\\n",
    "                                          .transform(ml_bucketized_features)\n",
    "\n",
    "# Check out the indexes\n",
    "ml_bucketized_features.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Numeric Fields into a Single Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|ArrDelayBucket|        Features_vec|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+\n",
      "|    13.0|2015-01-01 10:10:...|2015-01-01 07:30:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1024|   ABQ|ABQ-DFW|           2.0|[14.0,569.0,2.0,2...|\n",
      "|    17.0|2015-01-01 02:15:...|2014-12-31 23:25:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1184|   ABQ|ABQ-DFW|           2.0|[14.0,569.0,2.0,2...|\n",
      "|    36.0|2015-01-01 03:45:...|2015-01-01 01:00:...|     AA|         1|        4|        1|    -2.0| DFW|   569.0|2014-12-31|      336|   ABQ|ABQ-DFW|           3.0|[-2.0,569.0,2.0,2...|\n",
      "|   -21.0|2015-01-01 11:30:...|2015-01-01 09:55:...|     AA|         1|        4|        1|    -1.0| DFW|   731.0|2014-12-31|      125|   ATL|ATL-DFW|           0.0|[-1.0,731.0,2.0,2...|\n",
      "|   -14.0|2015-01-01 02:25:...|2015-01-01 00:55:...|     AA|         1|        4|        1|    -4.0| DFW|   731.0|2014-12-31|     1455|   ATL|ATL-DFW|           1.0|[-4.0,731.0,2.0,2...|\n",
      "|    16.0|2015-01-01 07:15:...|2015-01-01 05:45:...|     AA|         1|        4|        1|    15.0| DFW|   731.0|2014-12-31|     1473|   ATL|ATL-DFW|           2.0|[15.0,731.0,2.0,2...|\n",
      "|    -7.0|2015-01-01 04:15:...|2015-01-01 02:45:...|     AA|         1|        4|        1|    -2.0| DFW|   731.0|2014-12-31|     1513|   ATL|ATL-DFW|           1.0|[-2.0,731.0,2.0,2...|\n",
      "|    13.0|2015-01-01 08:50:...|2015-01-01 07:25:...|     AA|         1|        4|        1|     9.0| DFW|   731.0|2014-12-31|      194|   ATL|ATL-DFW|           2.0|[9.0,731.0,2.0,25...|\n",
      "|    25.0|2015-01-01 12:30:...|2015-01-01 11:00:...|     AA|         1|        4|        1|    -2.0| DFW|   731.0|2014-12-31|      232|   ATL|ATL-DFW|           2.0|[-2.0,731.0,2.0,2...|\n",
      "|    58.0|2015-01-01 13:40:...|2015-01-01 12:15:...|     AA|         1|        4|        1|    14.0| DFW|   731.0|2014-12-31|      276|   ATL|ATL-DFW|           3.0|[14.0,731.0,2.0,2...|\n",
      "|    14.0|2015-01-01 05:25:...|2015-01-01 03:55:...|     AA|         1|        4|        1|    15.0| DFW|   731.0|2014-12-31|      314|   ATL|ATL-DFW|           2.0|[15.0,731.0,2.0,2...|\n",
      "|     1.0|2015-01-01 10:05:...|2015-01-01 08:40:...|     AA|         1|        4|        1|    -5.0| DFW|   731.0|2014-12-31|      356|   ATL|ATL-DFW|           2.0|[-5.0,731.0,2.0,2...|\n",
      "|   -29.0|2015-01-01 02:12:...|2015-01-01 00:15:...|     AA|         1|        4|        1|    -9.0| MIA|   594.0|2014-12-31|     1652|   ATL|ATL-MIA|           0.0|[-9.0,594.0,2.0,2...|\n",
      "|   -10.0|2015-01-01 00:52:...|2014-12-31 23:00:...|     AA|         1|        4|        1|    -4.0| MIA|   594.0|2014-12-31|       17|   ATL|ATL-MIA|           1.0|[-4.0,594.0,2.0,2...|\n",
      "|    -3.0|2015-01-01 15:02:...|2015-01-01 13:10:...|     AA|         1|        4|        1|    -7.0| MIA|   594.0|2014-12-31|      349|   ATL|ATL-MIA|           1.0|[-7.0,594.0,2.0,2...|\n",
      "|    -8.0|2015-01-01 06:35:...|2015-01-01 05:30:...|     AA|         1|        4|        1|    -2.0| DFW|   190.0|2014-12-31|     1023|   AUS|AUS-DFW|           1.0|[-2.0,190.0,2.0,2...|\n",
      "|    -1.0|2014-12-31 22:50:...|2014-12-31 21:50:...|     AA|         1|        4|        1|    -2.0| DFW|   190.0|2014-12-31|     1178|   AUS|AUS-DFW|           1.0|[-2.0,190.0,2.0,2...|\n",
      "|   -14.0|2015-01-01 01:40:...|2015-01-01 00:30:...|     AA|         1|        4|        1|    -6.0| DFW|   190.0|2014-12-31|     1296|   AUS|AUS-DFW|           1.0|[-6.0,190.0,2.0,2...|\n",
      "|   -16.0|2015-01-01 02:15:...|2015-01-01 01:05:...|     AA|         1|        4|        1|    -4.0| DFW|   190.0|2014-12-31|     1356|   AUS|AUS-DFW|           0.0|[-4.0,190.0,2.0,2...|\n",
      "|    18.0|2015-01-01 08:55:...|2015-01-01 07:55:...|     AA|         1|        4|        1|     3.0| DFW|   190.0|2014-12-31|     1365|   AUS|AUS-DFW|           2.0|[3.0,190.0,2.0,25...|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle continuous, numeric fields by combining them into one feature vector\n",
    "numeric_columns = [\"DepDelay\", \"Distance\"]\n",
    "index_columns = [\"Carrier_index\", \"DayOfMonth_index\",\n",
    "                   \"DayOfWeek_index\", \"DayOfYear_index\", \"Origin_index\",\n",
    "                   \"Origin_index\", \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "  inputCols=numeric_columns + index_columns,\n",
    "  outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "# Drop the index columns\n",
    "for column in index_columns:\n",
    "  final_vectorized_features = final_vectorized_features.drop(column)\n",
    "\n",
    "# Check out the features\n",
    "final_vectorized_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Model in an Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5965632172973679\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|ArrDelayBucket|        Features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|   -11.0|2014-12-31 22:13:...|2014-12-31 21:10:...|     AA|         1|        4|        1|    -3.0| MIA|   192.0|2014-12-31|     1323|   MCO|MCO-MIA|           1.0|[-3.0,192.0,2.0,2...|[3.15496188966384...|[0.15774809448319...|       1.0|\n",
      "|    -3.0|2015-01-01 03:40:...|2015-01-01 02:00:...|     AS|         1|        4|        1|    -9.0| OME|   539.0|2014-12-31|      151|   ANC|ANC-OME|           1.0|[-9.0,539.0,9.0,2...|[4.02192166004895...|[0.20109608300244...|       1.0|\n",
      "|   -10.0|2015-01-01 13:10:...|2015-01-01 05:25:...|     VX|         1|        4|        1|     0.0| FLL|  2342.0|2014-12-31|      330|   LAX|LAX-FLL|           1.0|[0.0,2342.0,13.0,...|[4.40711487879725...|[0.22035574393986...|       1.0|\n",
      "|     3.0|2015-01-01 06:25:...|2015-01-01 05:25:...|     AA|         1|        4|        1|    -1.0| TUL|   237.0|2014-12-31|     1563|   DFW|DFW-TUL|           2.0|[-1.0,237.0,2.0,2...|[2.53084153459117...|[0.12654207672955...|       1.0|\n",
      "|   -14.0|2015-01-01 07:52:...|2015-01-01 06:38:...|     OO|         1|        4|        1|    -9.0| BUR|   326.0|2014-12-31|     6400|   SFO|SFO-BUR|           1.0|[-9.0,326.0,3.0,2...|[3.22466860417944...|[0.16123343020897...|       1.0|\n",
      "|     3.0|2015-01-01 15:19:...|2015-01-01 10:45:...|     B6|         1|        4|        1|    -2.0| BOS|  1698.0|2014-12-31|     1038|   AUS|AUS-BOS|           2.0|[-2.0,1698.0,7.0,...|[4.78266368992140...|[0.23913318449607...|       1.0|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cross validate, train and evaluate classifier\n",
    "#\n",
    "\n",
    "# Test/train split\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Instantiate and fit random forest classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "  featuresCol=\"Features_vec\",\n",
    "  labelCol=\"ArrDelayBucket\",\n",
    "  maxBins=4657,\n",
    "  maxMemoryInMB=1024\n",
    ")\n",
    "model = rfc.fit(training_data)\n",
    "\n",
    "# Evaluate model using test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "# Check a sample\n",
    "predictions.sample(False, 0.001, 18).orderBy(\"CRSDepTime\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
