{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spirit Airlines 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crisliu/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/crisliu/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlantic Southeast Airlines 4\n",
      "JetBlue Airways 5\n",
      "SkyWest 7\n",
      "Frontier Airlines 8\n",
      "American Eagle Airlines 10\n",
      "Virgin America 13\n",
      "Southwest Airlines 14\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re\n",
    "sys.path.append(\"lib\")\n",
    "import utils\n",
    "\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "# Load our airlines...\n",
    "our_airlines = utils.read_json_lines_file('../data/our_airlines.jsonl')\n",
    "\n",
    "# Build a new list that includes wikipedia data\n",
    "with_url = []\n",
    "i = 0 \n",
    "for airline in our_airlines:\n",
    "  i = i+1\n",
    "  if i == 1 or i ==3 or i == 6 or i == 9 or i==11 or i == 12:\n",
    "     continue\n",
    "\n",
    "  # Get the wikipedia page for the airline name\n",
    "  wikipage = wikipedia.page(airline['Name'])\n",
    "  print(airline['Name'],i)\n",
    "  # Get the summary\n",
    "  summary = wikipage.summary\n",
    "  airline['summary'] = summary\n",
    "\n",
    "  # Get the HTML of the page\n",
    "  page = BeautifulSoup(wikipage.html())\n",
    "\n",
    "  # Task: get the logo from the right 'vcard' column\n",
    "  # 1) Get the vcard table\n",
    "  vcard_table = page.find_all('table', class_='vcard')[0]\n",
    "  # 2) The logo is always the first image inside this table \n",
    "  first_image = vcard_table.find_all('img')[0]\n",
    "  # 3) Set the url to the image\n",
    "  logo_url = 'http:' + first_image.get('src')\n",
    "  airline['logo_url'] = logo_url\n",
    "\n",
    "  # Task: Get the company website\n",
    "  # 1) Find the 'Website' table header\n",
    "  th = page.find_all('th', text='Website')[0]\n",
    "  # 2) find the parent tr element\n",
    "  tr = th.parent\n",
    "  # 3) find the a (link) tag within the tr\n",
    "  a = tr.find_all('a')[0]\n",
    "  # 4) finally get the href of the a tag\n",
    "  url = a.get('href')\n",
    "  airline['url'] = url\n",
    "\n",
    "  # Get the domain to display with the url\n",
    "  url_parts = tldextract.extract(url)\n",
    "  airline['domain'] = url_parts.domain + '.' + url_parts.suffix\n",
    "\n",
    "  with_url.append(airline)\n",
    "\n",
    "utils.write_json_lines_file(with_url, '../data/our_airlines_with_wiki.jsonl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # 2) The logo is always the first image inside this table\n",
    "  lst = vcard_table.find_all('img')\n",
    "  if len(lst)>0:\n",
    "      first_image = lst[0] \n",
    "  # 3) Set the url to the image\n",
    "      logo_url = 'http:' + first_image.get('src')\n",
    "  else:\n",
    "      first_image= None\n",
    "      logo_url = None\n",
    "  airline['logo_url'] = logo_url\n",
    "   \n",
    "  # Task: Get the company website\n",
    "  # 1) Find the 'Website' table header\n",
    "  lst2 = page.find_all('th', text='Website')\n",
    "  th = page.find_all('th', text='Website')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -XPUT 'localhost:9200/agile_data_science?pretty' \\\n",
    "-H 'Content-Type: application/json' -d \\\n",
    "'{\"settings\" : {\n",
    "            \"index\" : {\n",
    "                \"number_of_shards\" : 1,\n",
    "                \"number_of_replicas\" : 1\n",
    "        } \n",
    "    } \n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -XPUT 'localhost:9200/agile_data_science/test/1?pretty' \\\n",
    "-H 'Content-Type: application/json' -d'\n",
    "    {\n",
    "        \"name\" : \"Russell Jurney\",\n",
    "        \"message\" : \"trying out Elasticsearch\"\n",
    "} '"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
